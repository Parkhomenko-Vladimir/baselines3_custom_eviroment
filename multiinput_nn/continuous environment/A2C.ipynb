{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from gym_environment import CustomEnv\n",
    "from network import CustomCNN\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collab callback class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model_A2C')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        pass\n",
    "        # Create folder if needed\n",
    "#         if self.save_path is not None:\n",
    "#             os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init logs, model, environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tatiana/environments/tatenv/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tatiana/environments/tatenv/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "log_dir = './saved_models_cont_mult/A2C/'\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
    "\n",
    "env = CustomEnv(obstacle_turn = False, \n",
    "                vizualaze     = False, \n",
    "                Total_war     = True)\n",
    "\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=518),\n",
    "    activation_fn=torch.nn.ReLU,\n",
    "    net_arch = [dict(pi=[1029, 128, 32, 8], vf=[1029, 128, 32, 8])])\n",
    "\n",
    "model = A2C(policy          = 'MlpPolicy',\n",
    "            env             = env,\n",
    "            learning_rate   = 0.0001,\n",
    "            n_steps         = 2,\n",
    "            gamma           = 0.99,\n",
    "            gae_lambda      = 0.95,\n",
    "            tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "            policy_kwargs   = policy_kwargs,\n",
    "            verbose         = 0,\n",
    "            device          = 'cuda',\n",
    "            use_sde         = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tatiana/environments/tatenv/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/tatiana/environments/tatenv/lib/python3.8/site-packages/torch/nn/modules/conv.py:439: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /pytorch/aten/src/ATen/native/Convolution.cpp:660.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -74.44\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -74.44 - Last mean reward per episode: -85.95\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -74.44 - Last mean reward per episode: -55.86\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -55.86 - Last mean reward per episode: -118.29\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -55.86 - Last mean reward per episode: -118.73\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -55.86 - Last mean reward per episode: -73.97\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -55.86 - Last mean reward per episode: -52.08\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -52.08 - Last mean reward per episode: -36.14\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -36.14 - Last mean reward per episode: -60.09\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -36.14 - Last mean reward per episode: -32.40\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 11000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -46.80\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -80.31\n",
      "Num timesteps: 13000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -111.05\n",
      "Num timesteps: 14000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -122.42\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -130.93\n",
      "Num timesteps: 16000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -135.03\n",
      "Num timesteps: 17000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -123.97\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -120.00\n",
      "Num timesteps: 19000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -125.06\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -134.98\n",
      "Num timesteps: 21000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -115.38\n",
      "Num timesteps: 22000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -113.87\n",
      "Num timesteps: 23000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -111.14\n",
      "Num timesteps: 24000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -104.96\n",
      "Num timesteps: 25000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -100.87\n",
      "Num timesteps: 26000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -102.38\n",
      "Num timesteps: 27000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -101.21\n",
      "Num timesteps: 28000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -98.84\n",
      "Num timesteps: 29000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -97.75\n",
      "Num timesteps: 30000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -114.45\n",
      "Num timesteps: 31000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -121.28\n",
      "Num timesteps: 32000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -111.73\n",
      "Num timesteps: 33000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -142.28\n",
      "Num timesteps: 34000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -142.43\n",
      "Num timesteps: 35000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -151.74\n",
      "Num timesteps: 36000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -148.48\n",
      "Num timesteps: 37000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -163.37\n",
      "Num timesteps: 38000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -149.44\n",
      "Num timesteps: 39000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -147.31\n",
      "Num timesteps: 40000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -134.05\n",
      "Num timesteps: 41000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -136.68\n",
      "Num timesteps: 42000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -141.70\n",
      "Num timesteps: 43000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -142.11\n",
      "Num timesteps: 44000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -131.03\n",
      "Num timesteps: 45000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -131.03\n",
      "Num timesteps: 46000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -144.11\n",
      "Num timesteps: 47000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -150.45\n",
      "Num timesteps: 48000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -138.06\n",
      "Num timesteps: 49000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -127.98\n",
      "Num timesteps: 50000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -141.57\n",
      "Num timesteps: 51000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -125.03\n",
      "Num timesteps: 52000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -143.37\n",
      "Num timesteps: 53000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -153.34\n",
      "Num timesteps: 54000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -151.04\n",
      "Num timesteps: 55000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -155.91\n",
      "Num timesteps: 56000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -155.18\n",
      "Num timesteps: 57000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -145.15\n",
      "Num timesteps: 58000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -140.62\n",
      "Num timesteps: 59000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -141.38\n",
      "Num timesteps: 60000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -137.14\n",
      "Num timesteps: 61000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -111.10\n",
      "Num timesteps: 62000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -89.12\n",
      "Num timesteps: 63000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -52.37\n",
      "Num timesteps: 64000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -55.68\n",
      "Num timesteps: 65000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -45.33\n",
      "Num timesteps: 66000\n",
      "Best mean reward: -32.40 - Last mean reward per episode: -31.13\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 67000\n",
      "Best mean reward: -31.13 - Last mean reward per episode: -31.13\n",
      "Num timesteps: 68000\n",
      "Best mean reward: -31.13 - Last mean reward per episode: -39.35\n",
      "Num timesteps: 69000\n",
      "Best mean reward: -31.13 - Last mean reward per episode: -32.68\n",
      "Num timesteps: 70000\n",
      "Best mean reward: -31.13 - Last mean reward per episode: -39.67\n",
      "Num timesteps: 71000\n",
      "Best mean reward: -31.13 - Last mean reward per episode: -32.55\n",
      "Num timesteps: 72000\n",
      "Best mean reward: -31.13 - Last mean reward per episode: -18.36\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 73000\n",
      "Best mean reward: -18.36 - Last mean reward per episode: 0.97\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 74000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 6.39\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 75000\n",
      "Best mean reward: 6.39 - Last mean reward per episode: 29.85\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 76000\n",
      "Best mean reward: 29.85 - Last mean reward per episode: 31.91\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 77000\n",
      "Best mean reward: 31.91 - Last mean reward per episode: 50.68\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 78000\n",
      "Best mean reward: 50.68 - Last mean reward per episode: 57.59\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 79000\n",
      "Best mean reward: 57.59 - Last mean reward per episode: 56.10\n",
      "Num timesteps: 80000\n",
      "Best mean reward: 57.59 - Last mean reward per episode: 74.31\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 81000\n",
      "Best mean reward: 74.31 - Last mean reward per episode: 74.31\n",
      "Num timesteps: 82000\n",
      "Best mean reward: 74.31 - Last mean reward per episode: 68.59\n",
      "Num timesteps: 83000\n",
      "Best mean reward: 74.31 - Last mean reward per episode: 78.51\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 84000\n",
      "Best mean reward: 78.51 - Last mean reward per episode: 75.67\n",
      "Num timesteps: 85000\n",
      "Best mean reward: 78.51 - Last mean reward per episode: 72.71\n",
      "Num timesteps: 86000\n",
      "Best mean reward: 78.51 - Last mean reward per episode: 72.71\n",
      "Num timesteps: 87000\n",
      "Best mean reward: 78.51 - Last mean reward per episode: 73.85\n",
      "Num timesteps: 88000\n",
      "Best mean reward: 78.51 - Last mean reward per episode: 73.85\n",
      "Num timesteps: 89000\n",
      "Best mean reward: 78.51 - Last mean reward per episode: 71.64\n",
      "Num timesteps: 90000\n",
      "Best mean reward: 78.51 - Last mean reward per episode: 62.89\n",
      "Num timesteps: 91000\n",
      "Best mean reward: 78.51 - Last mean reward per episode: 70.96\n",
      "Num timesteps: 92000\n",
      "Best mean reward: 78.51 - Last mean reward per episode: 68.86\n",
      "Num timesteps: 93000\n",
      "Best mean reward: 78.51 - Last mean reward per episode: 72.54\n",
      "Num timesteps: 94000\n",
      "Best mean reward: 78.51 - Last mean reward per episode: 72.54\n",
      "Num timesteps: 95000\n",
      "Best mean reward: 78.51 - Last mean reward per episode: 79.94\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 96000\n",
      "Best mean reward: 79.94 - Last mean reward per episode: 85.22\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 97000\n",
      "Best mean reward: 85.22 - Last mean reward per episode: 99.55\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 98000\n",
      "Best mean reward: 99.55 - Last mean reward per episode: 117.21\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 99000\n",
      "Best mean reward: 117.21 - Last mean reward per episode: 123.30\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 100000\n",
      "Best mean reward: 123.30 - Last mean reward per episode: 128.53\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 101000\n",
      "Best mean reward: 128.53 - Last mean reward per episode: 122.14\n",
      "Num timesteps: 102000\n",
      "Best mean reward: 128.53 - Last mean reward per episode: 122.51\n",
      "Num timesteps: 103000\n",
      "Best mean reward: 128.53 - Last mean reward per episode: 132.70\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 104000\n",
      "Best mean reward: 132.70 - Last mean reward per episode: 132.70\n",
      "Num timesteps: 105000\n",
      "Best mean reward: 132.70 - Last mean reward per episode: 135.46\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 106000\n",
      "Best mean reward: 135.46 - Last mean reward per episode: 136.40\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 107000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 130.41\n",
      "Num timesteps: 108000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 126.25\n",
      "Num timesteps: 109000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 121.37\n",
      "Num timesteps: 110000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 121.37\n",
      "Num timesteps: 111000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 116.97\n",
      "Num timesteps: 112000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 116.97\n",
      "Num timesteps: 113000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 111.29\n",
      "Num timesteps: 114000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 120.61\n",
      "Num timesteps: 115000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 117.38\n",
      "Num timesteps: 116000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 112.22\n",
      "Num timesteps: 117000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 113.91\n",
      "Num timesteps: 118000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 116.36\n",
      "Num timesteps: 119000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 119.84\n",
      "Num timesteps: 120000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 119.84\n",
      "Num timesteps: 121000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 106.68\n",
      "Num timesteps: 122000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 117.54\n",
      "Num timesteps: 123000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 111.54\n",
      "Num timesteps: 124000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 119.54\n",
      "Num timesteps: 125000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 122.68\n",
      "Num timesteps: 126000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 124.30\n",
      "Num timesteps: 127000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 122.34\n",
      "Num timesteps: 128000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 119.30\n",
      "Num timesteps: 129000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 119.30\n",
      "Num timesteps: 130000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 119.10\n",
      "Num timesteps: 131000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 128.71\n",
      "Num timesteps: 132000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 133.00\n",
      "Num timesteps: 133000\n",
      "Best mean reward: 136.40 - Last mean reward per episode: 148.94\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 134000\n",
      "Best mean reward: 148.94 - Last mean reward per episode: 141.78\n",
      "Num timesteps: 135000\n",
      "Best mean reward: 148.94 - Last mean reward per episode: 154.35\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 136000\n",
      "Best mean reward: 154.35 - Last mean reward per episode: 154.08\n",
      "Num timesteps: 137000\n",
      "Best mean reward: 154.35 - Last mean reward per episode: 154.08\n",
      "Num timesteps: 138000\n",
      "Best mean reward: 154.35 - Last mean reward per episode: 161.33\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 139000\n",
      "Best mean reward: 161.33 - Last mean reward per episode: 161.33\n",
      "Num timesteps: 140000\n",
      "Best mean reward: 161.33 - Last mean reward per episode: 154.62\n",
      "Num timesteps: 141000\n",
      "Best mean reward: 161.33 - Last mean reward per episode: 151.08\n",
      "Num timesteps: 142000\n",
      "Best mean reward: 161.33 - Last mean reward per episode: 145.53\n",
      "Num timesteps: 143000\n",
      "Best mean reward: 161.33 - Last mean reward per episode: 145.26\n",
      "Num timesteps: 144000\n",
      "Best mean reward: 161.33 - Last mean reward per episode: 165.58\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 145000\n",
      "Best mean reward: 165.58 - Last mean reward per episode: 165.58\n",
      "Num timesteps: 146000\n",
      "Best mean reward: 165.58 - Last mean reward per episode: 184.42\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 147000\n",
      "Best mean reward: 184.42 - Last mean reward per episode: 197.44\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 148000\n",
      "Best mean reward: 197.44 - Last mean reward per episode: 215.78\n",
      "Saving new best model to ./saved_models_cont_mult/A2C/best_model_A2C\n",
      "Num timesteps: 149000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 213.14\n",
      "Num timesteps: 150000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 211.50\n",
      "Num timesteps: 151000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 209.55\n",
      "Num timesteps: 152000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 208.76\n",
      "Num timesteps: 153000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 208.76\n",
      "Num timesteps: 154000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 201.07\n",
      "Num timesteps: 155000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 200.52\n",
      "Num timesteps: 156000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 193.26\n",
      "Num timesteps: 157000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 189.38\n",
      "Num timesteps: 158000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 189.38\n",
      "Num timesteps: 159000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 190.48\n",
      "Num timesteps: 160000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 189.06\n",
      "Num timesteps: 161000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 189.06\n",
      "Num timesteps: 162000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 176.73\n",
      "Num timesteps: 163000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 174.34\n",
      "Num timesteps: 164000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 172.52\n",
      "Num timesteps: 165000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 172.52\n",
      "Num timesteps: 166000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 166.32\n",
      "Num timesteps: 167000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 161.26\n",
      "Num timesteps: 168000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 161.26\n",
      "Num timesteps: 169000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 165.56\n",
      "Num timesteps: 170000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 162.39\n",
      "Num timesteps: 171000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 162.39\n",
      "Num timesteps: 172000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 164.13\n",
      "Num timesteps: 173000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 157.55\n",
      "Num timesteps: 174000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 154.56\n",
      "Num timesteps: 175000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 154.56\n",
      "Num timesteps: 176000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 149.06\n",
      "Num timesteps: 177000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 141.61\n",
      "Num timesteps: 178000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 141.61\n",
      "Num timesteps: 179000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 144.26\n",
      "Num timesteps: 180000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 139.31\n",
      "Num timesteps: 181000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 139.31\n",
      "Num timesteps: 182000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 137.79\n",
      "Num timesteps: 183000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 142.47\n",
      "Num timesteps: 184000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 142.47\n",
      "Num timesteps: 185000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 142.05\n",
      "Num timesteps: 186000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 138.51\n",
      "Num timesteps: 187000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 128.65\n",
      "Num timesteps: 188000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 127.43\n",
      "Num timesteps: 189000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 122.58\n",
      "Num timesteps: 190000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 107.91\n",
      "Num timesteps: 191000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 107.91\n",
      "Num timesteps: 192000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 106.66\n",
      "Num timesteps: 193000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 108.44\n",
      "Num timesteps: 194000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 108.44\n",
      "Num timesteps: 195000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 108.46\n",
      "Num timesteps: 196000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 96.21\n",
      "Num timesteps: 197000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 96.21\n",
      "Num timesteps: 198000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 99.55\n",
      "Num timesteps: 199000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 93.50\n",
      "Num timesteps: 200000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 93.50\n",
      "Num timesteps: 201000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 90.58\n",
      "Num timesteps: 202000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 91.60\n",
      "Num timesteps: 203000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 91.60\n",
      "Num timesteps: 204000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 88.99\n",
      "Num timesteps: 205000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 83.36\n",
      "Num timesteps: 206000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 83.36\n",
      "Num timesteps: 207000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 87.65\n",
      "Num timesteps: 208000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 87.46\n",
      "Num timesteps: 209000\n",
      "Best mean reward: 215.78 - Last mean reward per episode: 75.49\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1e6,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# env.render(model,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir ./tensorboard_logs_cont_mult/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and check the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './saved_models_cont_mult/A2C/'\n",
    "env = CustomEnv(obstacle_turn=False, Total_war=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C.load(path=log_dir + 'best_model_with_sde_2steps', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "action = model.predict(state)\n",
    "print(action[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velosities = []\n",
    "state = env.reset()\n",
    "action = model.predict(state)\n",
    "for i in range(0, 100):\n",
    "    state, reward, done, numstep = env.step(action[0])\n",
    "    action = model.predict(state)\n",
    "    velosities.append(action[0][0])\n",
    "print(max(velosities), min(velosities))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
