{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Enviroment import Enviroment\n",
    "import pygame\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self,obstacle_turn = False,vizualaze = True,Total_war = True,\n",
    "                 head_velocity = 0.01, num_obs = 0, num_enemy = 1, size_obs = [0, 0], steps_limit = 2000):\n",
    "        super(CustomEnv, self).__init__()\n",
    "\n",
    "#         obstacle_turn = False\n",
    "#         vizualaze = True\n",
    "#         Total_war = True\n",
    "        self.log_koef = 50\n",
    "        self.enviroment = Enviroment(obstacle_turn, vizualaze, Total_war, head_velocity, num_obs, num_enemy, size_obs, steps_limit)\n",
    "\n",
    "        state = self.enviroment.reset()\n",
    "\n",
    "        self.action_space = spaces.Discrete(8)\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "                    'img': spaces.Box(low=0, high=255, shape=(500, 500, 3), dtype=np.uint8),\n",
    "                    'posRobot': spaces.Box(low=np.array([0, 0,-3.14]), high=np.array([500, 500, 3.14])),\n",
    "                    'target': spaces.Box(low  = np.array([[0, 0,-3.14] for i in range(num_enemy)]).reshape(-1), \n",
    "                                         high = np.array([[500, 500, 3.14] for i in range(num_enemy)]).reshape(-1)\n",
    "                                        )\n",
    "                                                })\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        state, reward, done, numstep = self.enviroment.step(action)\n",
    "        dict_state = {'img':     state.img,\n",
    "                      'posRobot':state.posRobot,\n",
    "                      'target':  state.target} \n",
    "        for i in range(len(state.target)):\n",
    "        \n",
    "            dist = np.sqrt((dict_state['target'][i][0]-dict_state['posRobot'][0])**2 + (dict_state['target'][i][1]-dict_state['posRobot'][1])**2)\n",
    "\n",
    "            Ax = np.cos(dict_state['target'][i][2])\n",
    "            Ay = -np.sin(dict_state['target'][i][2])\n",
    "            Bx = dict_state['posRobot'][0] - dict_state['target'][i][0]\n",
    "            By = dict_state['posRobot'][1] - dict_state['target'][i][1] \n",
    "\n",
    "\n",
    "            phy = np.arccos((Ax*Bx + Ay*By)/(np.sqrt(Ax**2 + Ay**2) * np.sqrt(Bx**2 + By**2)))\n",
    "\n",
    "            reward = reward + np.log2(phy/dist*self.log_koef)\n",
    "            \n",
    "\n",
    "        dict_state = {'img':     state.img,\n",
    "                      'posRobot':state.posRobot,\n",
    "                      'target':  state.target.reshape(-1)} \n",
    "        return dict_state, reward, done, {}\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        state = self.enviroment.reset()\n",
    "        dict_state = {'img':     state.img,  \n",
    "                      'posRobot':state.posRobot,  \n",
    "                      'target':  state.target.reshape(-1)}  \n",
    "        return dict_state  # reward, done, info can't be included\n",
    "    \n",
    "    def render(self, model, num_gifs=1):\n",
    "        for i in range(num_gifs):\n",
    "            images = []\n",
    "            obs = self.reset()\n",
    "            img = obs['img']# env.render(mode='rgb_array')\n",
    "            done = False\n",
    "            \n",
    "            height, width, layers = img.shape\n",
    "            size = (width,height)\n",
    "            out = cv2.VideoWriter(f\"video{i}.avi\",cv2.VideoWriter_fourcc(*'DIVX'), 25, size)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            out.write(img)\n",
    "            while not done:\n",
    "\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, _, done ,_ = self.step(int(action))\n",
    "                img = obs['img']\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                out.write(img)\n",
    "        out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import gym\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Dict, features_dim: int = 518):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        \n",
    "        extractors = {}\n",
    "        \n",
    "        for key, subspace in observation_space.spaces.items():\n",
    "            if key == \"img\":\n",
    "        \n",
    "                n_input_channels = observation_space[key].shape[0]\n",
    "            \n",
    "                extractors[key] = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(n_input_channels, 32, 2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                nn.Conv2d(32, 64, 2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(4, 4),\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                ResBlock(n_filters=64, kernel_size=2), \n",
    "                nn.MaxPool2d(2, 2),\n",
    "                \n",
    "                nn.Conv2d(64, 128, 2),\n",
    "                nn.Flatten())\n",
    "                    \n",
    "            elif key == \"posRobot\":\n",
    "                            \n",
    "                extractors[key] = nn.Sequential(nn.Linear(3, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 3))\n",
    "                    \n",
    "            elif key == \"target\":\n",
    "                            \n",
    "                extractors[key] = nn.Sequential(nn.Linear(3, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 3))\n",
    "                \n",
    "        self.extractors = nn.ModuleDict(extractors)\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        encoded_tensor_list = []\n",
    "\n",
    "        # self.extractors contain nn.Modules that do all the processing.\n",
    "        for key, extractor in self.extractors.items():\n",
    "            encoded_tensor_list.append(extractor(observations[key]))\n",
    "        # Return a (B, self._features_dim) PyTorch tensor, where B is batch dimension.\n",
    "        return th.cat(encoded_tensor_list, dim=1)\n",
    "\n",
    "\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_filters, kernel_size):\n",
    "        super().__init__()\n",
    "        self.n_filters = n_filters # the number of filters represents the number of output channels AND the number of input channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.b1 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "        \n",
    "        self.b2 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99) # in keras the default epsilon = 0.001 & momentum = 0.99\n",
    "        # whereas in putorch the default epsilon = 1e-05 & momentom = 0.1, the output is the same shape of input (N,C,H,W).\n",
    "        self.b3 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "        self.b4 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        y = F.relu(self.b1(x))\n",
    "        y = self.b2(y)\n",
    "        y = F.relu(self.b3(y))\n",
    "        y = self.b4(y)\n",
    "        y += residual\n",
    "        y = F.relu(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, agent_name: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model_' + agent_name)\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim = 518),\n",
    "    activation_fn=torch.nn.ReLU,\n",
    "    net_arch = (518,128, 32, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv(obstacle_turn = True, \n",
    "                vizualaze     = True, \n",
    "                Total_war     = True,\n",
    "                head_velocity = 0.01,\n",
    "                num_obs       = 6, \n",
    "                num_enemy     = 1, \n",
    "                size_obs      = [10, 20],\n",
    "                steps_limit   = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN  # \n",
    "\n",
    "log_dir = './saved_models_disc_mult/DQN_with/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq  = 1000, \n",
    "                                            log_dir     = log_dir,\n",
    "                                            agent_name  = 'DQN')\n",
    "\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "model = DQN(policy                  = 'MlpPolicy',\n",
    "            env                     = env,\n",
    "            learning_rate           = 0.0001,\n",
    "            buffer_size             = 10000,\n",
    "            batch_size              = 20,\n",
    "            gamma                   = 0.99,\n",
    "            tensorboard_log         = \"./tensorboard_logs_disc_mult/\",\n",
    "            policy_kwargs           = policy_kwargs,\n",
    "            verbose                 = 0,\n",
    "            device                  = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1e6,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Записать видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render(model,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir ./tensorboard_logs_disc_mult/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO.load(\"./saved_models_disc_mult/TD3/best_model_PPo1\", env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_action = 1\n",
    "# while not done:\n",
    "#     state, reward, done, numstep = env.step(last_action)\n",
    "# #     print(reward)#, done, state['target'])\n",
    "#     action = input('action')\n",
    "#     if action:\n",
    "#         last_action = int(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
