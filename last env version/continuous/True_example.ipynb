{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Enviroment import Enviroment\n",
    "import pygame\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import cv2 \n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    '''\n",
    "    Оборочивание класса среды в среду gym\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, obstacle_turn: bool, Total_war: bool, num_obs: int, num_enemy: int, \n",
    "                 size_obs, steps_limit, vizualaze=False, head_velocity=0.01):\n",
    "        '''\n",
    "        Инициализация класса среды\n",
    "        :param obstacle_turn: (bool) Флаг генерации препятствий\n",
    "        :param vizualaze: (bool) Флаг генерации препятствий\n",
    "        :param Total_war: (bool) Флаг режима игры (с противником или без)\n",
    "        :param steps_limit: (int) Максимальное количество действий в среде за одну игру\n",
    "        '''\n",
    "\n",
    "        self.velocity = 70\n",
    "        self.log_koef = 50\n",
    "\n",
    "        self.enviroment = Enviroment(obstacle_turn, vizualaze, Total_war,\n",
    "                                     head_velocity, num_obs, num_enemy, size_obs, steps_limit)\n",
    "\n",
    "        self.enviroment.reset()\n",
    "\n",
    "        self.action_space = spaces.Box(low=np.array([-0.1, -3.14]), high=np.array([1, 3.14]), dtype=np.float16)\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "                    'img': spaces.Box(low=0, high=255, shape=(500, 500, 3), dtype=np.uint8),\n",
    "                    'posRobot': spaces.Box(low=np.array([0, 0,-3.14]), high=np.array([500, 500, 3.14])),\n",
    "                    'target': spaces.Box(low  = np.array([[0, 0,-3.14] for i in range(num_enemy)]).reshape(-1), \n",
    "                                         high = np.array([[500, 500, 3.14] for i in range(num_enemy)]).reshape(-1)\n",
    "                                        )\n",
    "                                                })\n",
    "        self.img1 = None\n",
    "        self.img2 = None\n",
    "        self.img3 = None\n",
    "\n",
    "    def make_layers(self):\n",
    "        \"\"\"\n",
    "        Функция наслоения изображений трех последовательных шагов в среде\n",
    "        :param img1, img2, img3: состояния среды на трех последовательных шагах\n",
    "        :return: new_img: изображение, содержащее информацию о состояниях среды на трех последовательных шагах, отображенную с разной интенсивностью\n",
    "        \"\"\"\n",
    "        new_img = cv2.addWeighted(self.img2, 0.4, self.img1, 0.2, 0)\n",
    "        self.Img = cv2.addWeighted(self.img3, 0.7, new_img, 0.5, 0)\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Метод осуществления шага в среде\n",
    "        :param action: (int) направление движения в среде\n",
    "        :return: dict_state, reward, not done, {}: состояние, реворд, флаг терминального состояния, информация о среде\n",
    "        \"\"\"\n",
    "        \n",
    "        action[0] *= self.velocity\n",
    "        state, reward, done, numstep = self.enviroment.step(action)\n",
    "        \n",
    "        self.img1 = self.img2\n",
    "        self.img2 = self.img3\n",
    "        self.img3 = state.img\n",
    "        \n",
    "        self.make_layers()\n",
    "        \n",
    "        dist = np.sqrt((state.target[:,0]-state.posRobot[0])**2 + (state.target[:,1]-state.posRobot[1])**2) \n",
    "        Ax = np.cos(state.target[:,2])\n",
    "        Ay = -np.sin(state.target[:,2])\n",
    "        Bx = state.posRobot[0] - state.target[:,0]\n",
    "        By = state.posRobot[1] - state.target[:,1]\n",
    "\n",
    "        phy = np.arccos((Ax*Bx + Ay*By)/(np.sqrt(Ax**2 + Ay**2) * np.sqrt(Bx**2 + By**2)))\n",
    "        np.clip(dist, 1e-9, None)\n",
    "        \n",
    "        reward += np.sum(np.log2(phy/dist*self.log_koef)) * int(not done)\n",
    "        \n",
    "        dict_state = {'img':     self.Img,\n",
    "                      'posRobot':state.posRobot,\n",
    "                      'target':  state.target.reshape(-1)}\n",
    "\n",
    "        return dict_state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Метод обновления игры\n",
    "        :return: dict_state: состояние\n",
    "        '''\n",
    "        \n",
    "        state = self.enviroment.reset()\n",
    "        self.img2 = state.img\n",
    "        self.img3 = state.img\n",
    "        dict_state = {'img':     state.img,  \n",
    "                      'posRobot':state.posRobot,  \n",
    "                      'target':  state.target.reshape(-1)}\n",
    "\n",
    "        return dict_state\n",
    "\n",
    "    def render(self, model, num_gifs=1):\n",
    "        '''\n",
    "        Метод вывода информации об игре\n",
    "        :param mode:\n",
    "        :return:\n",
    "        '''\n",
    "        for i in range(num_gifs):\n",
    "            \n",
    "            images = []\n",
    "            obs = self.reset()\n",
    "            img = obs['img']# env.render(mode='rgb_array')\n",
    "            done = False\n",
    "                \n",
    "            height, width, layers = img.shape\n",
    "            size = (width,height)\n",
    "            out = cv2.VideoWriter(f\"video{i}.avi\",cv2.VideoWriter_fourcc(*'DIVX'), 25, size)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            out.write(img)\n",
    "            while not done:\n",
    "\n",
    "                action, _ = model.predict(obs)\n",
    "                print(action)\n",
    "                obs, _, done ,_ = self.step(action)\n",
    "                img = obs['img']\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                out.write(img)\n",
    "            out.release()\n",
    "    \n",
    "    def get_statistic(self, model, num_games):\n",
    "        collision = 0\n",
    "        win = 0\n",
    "        destroyed = 0\n",
    "        loss = 0\n",
    "        \n",
    "        pbar = tqdm(range(num_games))\n",
    "        for i in pbar:\n",
    "            obs = self.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, reward, done ,_ = self.step(action)\n",
    "                \n",
    "                \n",
    "                \n",
    "            if reward == -30:#win\n",
    "                collision+=1\n",
    "            elif reward == 100:# loss\n",
    "                win +=1\n",
    "            elif reward == -100:# loss\n",
    "                destroyed +=1\n",
    "            else:    #not_achieved\n",
    "                loss+=1\n",
    "        \n",
    "        print(\"Win: \",win/num_games)\n",
    "        print(\"destroyed: \", destroyed/num_games)\n",
    "        print(\"loss: \",loss/num_games)\n",
    "        print(\"collision: \",collision/num_games)\n",
    "        \n",
    "        \n",
    "        \n",
    "#     def statistic_study(self, model, num_games):\n",
    "#         enemy_num = 0\n",
    "#         success = 0\n",
    "#         pbar = tqdm(range(num_games))\n",
    "#         for i in pbar:\n",
    " \n",
    "#             pbar.set_description(f\"Processing {i}\")\n",
    "#             obs = self.reset()\n",
    "#             done = False\n",
    "#             while not done:\n",
    "#                 action, _ = model.predict(obs)\n",
    "#                 obs, reward, done ,_ = self.step(action)\n",
    "#                 if self.enviroment.success_mission == True:\n",
    "#                     enemy_num +=1\n",
    "#             if done == True and self.enviroment.success_mission == True:\n",
    "#                 success +=1\n",
    "#             pbar.set_postfix({'success episode': success, 'success(%)': (success*100)/(i+1)})\n",
    "                \n",
    "#         succ_percentage = success / num_games * 100\n",
    "#         print(\"success persentage: \",succ_percentage)\n",
    "#         print(\"number of enemy taken down: \", enemy_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Dict, features_dim: int = 518):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        \n",
    "        extractors = {}\n",
    "        \n",
    "        for key, subspace in observation_space.spaces.items():\n",
    "            if key == \"img\":\n",
    "        \n",
    "                n_input_channels = observation_space[key].shape[0]\n",
    "            \n",
    "                extractors[key] = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(n_input_channels, 32, 2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                nn.Conv2d(32, 64, 2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(4, 4),\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                ResBlock(n_filters=64, kernel_size=2), \n",
    "                nn.MaxPool2d(2, 2),\n",
    "                \n",
    "                nn.Conv2d(64, 128, 2),\n",
    "                nn.Flatten())\n",
    "                    \n",
    "            elif key == \"posRobot\":\n",
    "                \n",
    "                n_input_channels = observation_space[key].shape[0]\n",
    "                \n",
    "                extractors[key] = nn.Sequential(nn.Linear(n_input_channels, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 3))\n",
    "                    \n",
    "            elif key == \"target\":\n",
    "                            \n",
    "                n_input_channels = observation_space[key].shape[0]\n",
    "                    \n",
    "                extractors[key] = nn.Sequential(nn.Linear(n_input_channels, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 3))\n",
    "                \n",
    "        self.extractors = nn.ModuleDict(extractors)\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        '''\n",
    "        Forward propagation\n",
    "        :param observations: (dict) изображение; координаты и углы ориентации агентов\n",
    "        :return: features tensor\n",
    "        '''\n",
    "        encoded_tensor_list = []\n",
    "\n",
    "        for key, extractor in self.extractors.items():\n",
    "            encoded_tensor_list.append(extractor(observations[key]))\n",
    "\n",
    "        return th.cat(encoded_tensor_list, dim=1)\n",
    "\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_filters, kernel_size):\n",
    "        \"\"\"\n",
    "        Инициализация кастомного резнетовского блока\n",
    "        :param n_filters: (int) количество фильтров сверточного слоя\n",
    "        :param kernel_size: (int) размер ядра свертки\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.b1 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "    \n",
    "        self.b2 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99)\n",
    "        self.b3 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "        self.b4 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward propagation\n",
    "        :param x: input\n",
    "        :return: output\n",
    "        '''\n",
    "        residual = x\n",
    "        y = F.relu(self.b1(x))\n",
    "        y = self.b2(y)\n",
    "        y = F.relu(self.b3(y))\n",
    "        y = self.b4(y)\n",
    "        y += residual\n",
    "        y = F.relu(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, agent_name: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model_')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        pass\n",
    "        # Create folder if needed\n",
    "#         if self.save_path is not None:\n",
    "#             os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=518),\n",
    "    activation_fn=torch.nn.ReLU,\n",
    "    net_arch = [dict(pi=[1029, 128, 32, 8], vf=[1029, 128, 32, 8])])  # for metods with 2 neural networks\n",
    "#     net_arch = [64, 64])  # for metods with 1 neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv(obstacle_turn = False, \n",
    "                vizualaze     = False, \n",
    "                Total_war     = True,\n",
    "                head_velocity = 0.005,\n",
    "                num_obs       = 5, \n",
    "                num_enemy     = 1, \n",
    "                size_obs = [30, 40],\n",
    "                steps_limit    = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, TD3, DDPG, SAC\n",
    "\n",
    "log_dir = './saved_models_cont/PPO/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq  = 5000, \n",
    "                                            log_dir     = log_dir,\n",
    "                                            agent_name  = 'PPO')\n",
    "\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "model = PPO(policy          = 'MlpPolicy',  # 2 neural networks metod\n",
    "            env             = env,\n",
    "            learning_rate   = 0.0001,\n",
    "            n_steps         = 2048, \n",
    "            batch_size      = 24,\n",
    "            tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "            policy_kwargs   = policy_kwargs,\n",
    "            verbose         = 0,\n",
    "            device          = 'cuda')\n",
    "\n",
    "# model = A2C(policy          = 'MlpPolicy',  # 2 neural networks metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             n_steps         = 10,\n",
    "#             gamma           = 0.99,\n",
    "#             gae_lambda      = 0.95,\n",
    "#             tensorboard_log = \"./tensorboard_logs_disc_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')\n",
    "\n",
    "# model = TD3(policy          = 'MlpPolicy',  # 1 neural network metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             buffer_size     = 100,\n",
    "#             batch_size      = 2,\n",
    "#             gamma           = 0.99,\n",
    "#             tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')\n",
    "\n",
    "# model = DDPG(policy         = 'MlpPolicy',  # 1 neural network metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             buffer_size     = 100,\n",
    "#             batch_size      = 2,\n",
    "#             gamma           = 0.99,\n",
    "#             tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')\n",
    "\n",
    "# model = SAC(policy          = 'MlpPolicy',  # 1 neural network metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             buffer_size     = 100,\n",
    "#             batch_size      = 2,\n",
    "#             gamma           = 0.99,\n",
    "#             tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1e6,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# env.render(model,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the statistics of playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_statistic(model, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tensorboard check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir ./tensorboard_logs_cont_mult/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = PPO.load(\"./saved_models_cont/PPO/best_model_\", env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User mode of play to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# last_action = [0.4, -np.pi/2]\n",
    "# while not done:\n",
    "#     state, reward, done, numstep = env.step(last_action)\n",
    "#     print(reward)#, done, state['target'])\n",
    "#     action = input('theta')\n",
    "#     if action:\n",
    "#         last_action[1] = float(action)\n",
    "#         action = input('velocity')\n",
    "#         if action:\n",
    "#             last_action[0] = float(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
