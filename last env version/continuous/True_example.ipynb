{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from Enviroment import Enviroment\n",
    "import pygame\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import cv2 \n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    '''\n",
    "    Оборочивание класса среды в среду gym\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, obstacle_turn: bool, Total_war: bool, num_obs: int, num_enemy: int, \n",
    "                 size_obs, steps_limit, vizualaze=False, head_velocity=0.01):\n",
    "        '''\n",
    "        Инициализация класса среды\n",
    "        :param obstacle_turn: (bool) Флаг генерации препятствий\n",
    "        :param vizualaze: (bool) Флаг генерации препятствий\n",
    "        :param Total_war: (bool) Флаг режима игры (с противником или без)\n",
    "        :param steps_limit: (int) Максимальное количество действий в среде за одну игру\n",
    "        '''\n",
    "        self.log_koef = 50\n",
    "\n",
    "        self.velocity_coef = 35       #  1/2 max speed !!!\n",
    "        self.ang_Norm_coef = np.pi\n",
    "        self.coords_Norm_coef = 500\n",
    "        \n",
    "        self.enviroment = Enviroment(obstacle_turn, vizualaze, Total_war,\n",
    "                                     head_velocity, num_obs, num_enemy, size_obs, steps_limit)\n",
    "\n",
    "        self.enviroment.reset()\n",
    "\n",
    "        self.action_space = spaces.Box(low=np.array([-1, -1]), high=np.array([1, 1]), dtype=np.float16)\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "                    'img': spaces.Box(low=0, high=255, shape=(500, 500, 3), dtype=np.uint8),\n",
    "                    'posRobot': spaces.Box(low=np.array([0, 0,-3.14]), high=np.array([500, 500, 3.14])),\n",
    "                    'target': spaces.Box(low  = np.array([[0, 0,-3.14] for i in range(num_enemy)]).reshape(-1), \n",
    "                                         high = np.array([[500, 500, 3.14] for i in range(num_enemy)]).reshape(-1)\n",
    "                                        )\n",
    "                                                })\n",
    "        if num_enemy > 1:\n",
    "            self.normTarget = self._NormSomeCoords\n",
    "        else:\n",
    "            self.normTarget = self._NormOneCoords\n",
    "        \n",
    "        self.poseRobot = self._NormOneCoords\n",
    "        \n",
    "        self.img1 = None\n",
    "        self.img2 = None\n",
    "        self.img3 = None\n",
    "        \n",
    "\n",
    "    def make_layers(self):\n",
    "        \"\"\"\n",
    "        Функция наслоения изображений трех последовательных шагов в среде\n",
    "        :param img1, img2, img3: состояния среды на трех последовательных шагах\n",
    "        :return: new_img: изображение, содержащее информацию о состояниях среды на трех последовательных шагах, отображенную с разной интенсивностью\n",
    "        \"\"\"\n",
    "        new_img = cv2.addWeighted(self.img2, 0.4, self.img1, 0.2, 0)\n",
    "        self.Img = cv2.addWeighted(self.img3, 0.7, new_img, 0.5, 0)\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Метод осуществления шага в среде\n",
    "        :param action: (int) направление движения в среде\n",
    "        :return: dict_state, reward, not done, {}: состояние, реворд, флаг терминального состояния, информация о среде\n",
    "        \"\"\"\n",
    "        \n",
    "        action[0] *= self.velocity_coef\n",
    "        action[0] += self.velocity_coef \n",
    "        action[1] *= self.ang_Norm_coef\n",
    "        \n",
    "        state, reward, done, numstep = self.enviroment.step(action)\n",
    "        \n",
    "        self.img1 = self.img2\n",
    "        self.img2 = self.img3\n",
    "        self.img3 = state.img\n",
    "        \n",
    "        self.make_layers()\n",
    "    \n",
    "        dist = np.sqrt((state.target[:,0]-state.posRobot[0])**2 + (state.target[:,1]-state.posRobot[1])**2) \n",
    "        Ax = np.cos(state.target[:,2])\n",
    "        Ay = -np.sin(state.target[:,2])\n",
    "        Bx = state.posRobot[0] - state.target[:,0]\n",
    "        By = state.posRobot[1] - state.target[:,1]\n",
    "\n",
    "        phy = np.arccos((Ax*Bx + Ay*By)/(np.sqrt(Ax**2 + Ay**2) * np.sqrt(Bx**2 + By**2)))\n",
    "        np.clip(dist, 1e-9, None)\n",
    "        \n",
    "        reward += np.sum(np.log2(phy/dist*self.log_koef)) * int(not done)\n",
    "\n",
    "        \n",
    "        dict_state = {'img':     state.img,  \n",
    "                      'posRobot':self.poseRobot(state.posRobot),  \n",
    "                      'target':  self.normTarget(state.target).reshape(-1)}\n",
    "\n",
    "        return dict_state, reward, done, {}\n",
    "    \n",
    "    def _NormSomeCoords(self, coords):\n",
    "        '''\n",
    "        Метод нормализации координат\n",
    "        :return: coords: нормализованные координаты\n",
    "        '''\n",
    "        coords=np.float32(coords)\n",
    "        coords[:,2]  = coords[:,2] / self.ang_Norm_coef #угол\n",
    "        coords[:,:2] = coords[:,:2] / self.coords_Norm_coef #координаты\n",
    "        \n",
    "        return coords\n",
    "\n",
    "    def _NormOneCoords(self, coords):\n",
    "        '''\n",
    "        Метод нормализации координат\n",
    "        :return: coords: нормализованные координаты\n",
    "        '''\n",
    "        coords=np.float32(coords)\n",
    "        coords[2]  = coords[2] / self.ang_Norm_coef #угол\n",
    "        coords[:2] = coords[:2] / self.coords_Norm_coef #координаты\n",
    "        \n",
    "        return coords\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Метод обновления игры\n",
    "        :return: dict_state: состояние\n",
    "        '''\n",
    "        \n",
    "        state = self.enviroment.reset()\n",
    "        \n",
    "        self.img2 = state.img\n",
    "        self.img3 = state.img\n",
    "        \n",
    "        dict_state = {'img':     state.img,  \n",
    "                      'posRobot':self.poseRobot(state.posRobot),  \n",
    "                      'target':  self.normTarget(state.target).reshape(-1)}\n",
    "\n",
    "        return dict_state\n",
    "\n",
    "    def render(self, model, num_gifs=1):\n",
    "        '''\n",
    "        Метод вывода информации об игре\n",
    "        :param mode:\n",
    "        :return:\n",
    "        '''\n",
    "        for i in range(num_gifs):\n",
    "            \n",
    "            images = []\n",
    "            obs = self.reset()\n",
    "            img = obs['img']# env.render(mode='rgb_array')\n",
    "            done = False\n",
    "                \n",
    "            height, width, layers = img.shape\n",
    "            size = (width,height)\n",
    "            out = cv2.VideoWriter(f\"video{i}.avi\",cv2.VideoWriter_fourcc(*'DIVX'), 25, size)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            out.write(img)\n",
    "            while not done:\n",
    "\n",
    "                action, _ = model.predict(obs)\n",
    "                print(action)\n",
    "                obs, _, done ,_ = self.step(action)\n",
    "                img = obs['img']\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                out.write(img)\n",
    "            out.release()\n",
    "    \n",
    "    def get_statistic(self, model, num_games):\n",
    "        collision = 0\n",
    "        win = 0\n",
    "        destroyed = 0\n",
    "        loss = 0\n",
    "        \n",
    "        pbar = tqdm(range(num_games))\n",
    "        for i in pbar:\n",
    "            obs = self.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, reward, done ,_ = self.step(action)\n",
    "                \n",
    "                \n",
    "                \n",
    "            if reward == -30:#win\n",
    "                collision+=1\n",
    "            elif reward == 100:# loss\n",
    "                win +=1\n",
    "            elif reward == -100:# loss\n",
    "                destroyed +=1\n",
    "            else:    #not_achieved\n",
    "                loss+=1\n",
    "        \n",
    "        print(\"Win: \",win/num_games)\n",
    "        print(\"destroyed: \", destroyed/num_games)\n",
    "        print(\"loss: \",loss/num_games)\n",
    "        print(\"collision: \",collision/num_games)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Dict, features_dim: int = 518):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        \n",
    "        extractors = {}\n",
    "        \n",
    "        for key, subspace in observation_space.spaces.items():\n",
    "            if key == \"img\":\n",
    "        \n",
    "                n_input_channels = observation_space[key].shape[0]\n",
    "            \n",
    "                extractors[key] = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(n_input_channels, 32, 2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                nn.Conv2d(32, 64, 2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(4, 4),\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                ResBlock(n_filters=64, kernel_size=2), \n",
    "                nn.MaxPool2d(2, 2),\n",
    "                \n",
    "                nn.Conv2d(64, 128, 2),\n",
    "                nn.Flatten())\n",
    "                    \n",
    "            elif key == \"posRobot\":\n",
    "                \n",
    "                n_input_channels = observation_space[key].shape[0]\n",
    "                \n",
    "                extractors[key] = nn.Sequential(nn.Linear(n_input_channels, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 3))\n",
    "                    \n",
    "            elif key == \"target\":\n",
    "                            \n",
    "                n_input_channels = observation_space[key].shape[0]\n",
    "                    \n",
    "                extractors[key] = nn.Sequential(nn.Linear(n_input_channels, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 3))\n",
    "                \n",
    "        self.extractors = nn.ModuleDict(extractors)\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        '''\n",
    "        Forward propagation\n",
    "        :param observations: (dict) изображение; координаты и углы ориентации агентов\n",
    "        :return: features tensor\n",
    "        '''\n",
    "        encoded_tensor_list = []\n",
    "\n",
    "        for key, extractor in self.extractors.items():\n",
    "            encoded_tensor_list.append(extractor(observations[key]))\n",
    "\n",
    "        return th.cat(encoded_tensor_list, dim=1)\n",
    "\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_filters, kernel_size):\n",
    "        \"\"\"\n",
    "        Инициализация кастомного резнетовского блока\n",
    "        :param n_filters: (int) количество фильтров сверточного слоя\n",
    "        :param kernel_size: (int) размер ядра свертки\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.b1 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "    \n",
    "        self.b2 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99)\n",
    "        self.b3 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "        self.b4 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward propagation\n",
    "        :param x: input\n",
    "        :return: output\n",
    "        '''\n",
    "        residual = x\n",
    "        y = F.relu(self.b1(x))\n",
    "        y = self.b2(y)\n",
    "        y = F.relu(self.b3(y))\n",
    "        y = self.b4(y)\n",
    "        y += residual\n",
    "        y = F.relu(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, agent_name: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model_')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        pass\n",
    "        # Create folder if needed\n",
    "#         if self.save_path is not None:\n",
    "#             os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=518),\n",
    "    activation_fn=torch.nn.ReLU,\n",
    "    net_arch = [dict(pi=[1029, 128, 32, 8], vf=[1029, 128, 32, 8])])  # for metods with 2 neural networks\n",
    "#     net_arch = [64, 64])  # for metods with 1 neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/user/anaconda3/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = CustomEnv(obstacle_turn = True, \n",
    "                vizualaze     = False, \n",
    "                Total_war     = True,\n",
    "                head_velocity = 0.005,\n",
    "                num_obs       = 4, \n",
    "                num_enemy     = 2, \n",
    "                size_obs = [30, 40],\n",
    "                steps_limit    = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:137: UserWarning: You have specified a mini-batch size of 24, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2048`, after every 85 untruncated mini-batches, there will be a truncated mini-batch of size 8\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2048 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, A2C, TD3, DDPG, SAC\n",
    "\n",
    "log_dir = './saved_models_cont/PPO/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq  = 5000, \n",
    "                                            log_dir     = log_dir,\n",
    "                                            agent_name  = 'PPO')\n",
    "\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "model = PPO(policy          = 'MlpPolicy',  # 2 neural networks metod\n",
    "            env             = env,\n",
    "            learning_rate   = 0.0001,\n",
    "            n_steps         = 2048, \n",
    "            batch_size      = 24,\n",
    "            tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "            policy_kwargs   = policy_kwargs,\n",
    "            verbose         = 0,\n",
    "            device          = 'cuda')\n",
    "\n",
    "# model = A2C(policy          = 'MlpPolicy',  # 2 neural networks metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             n_steps         = 10,\n",
    "#             gamma           = 0.99,\n",
    "#             gae_lambda      = 0.95,\n",
    "#             tensorboard_log = \"./tensorboard_logs_disc_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')\n",
    "\n",
    "# model = TD3(policy          = 'MlpPolicy',  # 1 neural network metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             buffer_size     = 100,\n",
    "#             batch_size      = 2,\n",
    "#             gamma           = 0.99,\n",
    "#             tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')\n",
    "\n",
    "# model = DDPG(policy         = 'MlpPolicy',  # 1 neural network metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             buffer_size     = 100,\n",
    "#             batch_size      = 2,\n",
    "#             gamma           = 0.99,\n",
    "#             tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')\n",
    "\n",
    "# model = SAC(policy          = 'MlpPolicy',  # 1 neural network metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             buffer_size     = 100,\n",
    "#             batch_size      = 2,\n",
    "#             gamma           = 0.99,\n",
    "#             tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/user/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:439: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/aten/src/ATen/native/Convolution.cpp:660.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5000\n",
      "Best mean reward: -inf - Last mean reward per episode: -422.76\n",
      "Saving new best model to ./saved_models_cont/PPO/best_model_\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -422.76 - Last mean reward per episode: -405.51\n",
      "Saving new best model to ./saved_models_cont/PPO/best_model_\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -405.51 - Last mean reward per episode: -437.30\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -405.51 - Last mean reward per episode: -466.39\n",
      "Num timesteps: 25000\n",
      "Best mean reward: -405.51 - Last mean reward per episode: -376.67\n",
      "Saving new best model to ./saved_models_cont/PPO/best_model_\n",
      "Num timesteps: 30000\n",
      "Best mean reward: -376.67 - Last mean reward per episode: -297.15\n",
      "Saving new best model to ./saved_models_cont/PPO/best_model_\n",
      "Num timesteps: 35000\n",
      "Best mean reward: -297.15 - Last mean reward per episode: -354.21\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1e6,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# env.render(model,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the statistics of playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_statistic(model, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tensorboard check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir ./tensorboard_logs_cont_mult/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = PPO.load(\"./saved_models_cont/PPO/best_model_\", env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User mode of play to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# last_action = [0.4, -np.pi/2]\n",
    "# while not done:\n",
    "#     state, reward, done, numstep = env.step(last_action)\n",
    "#     print(reward)#, done, state['target'])\n",
    "#     action = input('theta')\n",
    "#     if action:\n",
    "#         last_action[1] = float(action)\n",
    "#         action = input('velocity')\n",
    "#         if action:\n",
    "#             last_action[0] = float(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
