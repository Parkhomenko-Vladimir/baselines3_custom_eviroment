{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Enviroment import Enviroment\n",
    "import pygame\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, obstacle_turn = False, vizualaze = False, Total_war = True, head_velocity = 0.01,steps_limit = 2000):\n",
    "        super(CustomEnv, self).__init__()\n",
    "\n",
    "\n",
    "        self.steps_limit = steps_limit\n",
    "        \n",
    "        self.env1 = Enviroment(obstacle_turn, vizualaze, Total_war, head_velocity)\n",
    "        state = self.env1.reset()\n",
    "\n",
    "        self.action_space = spaces.Discrete(8)\n",
    "        \n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "                    'img': spaces.Box(low=0, high=255, shape=(500, 500, 3), dtype=np.uint8),\n",
    "                    'posRobot': spaces.Box(low=np.array([0, 0,-3.14]), high=np.array([500, 500, 3.14])),\n",
    "                    'target': spaces.Box(low=np.array([0, 0,-3.14]), high=np.array([500, 500, 3.14]))\n",
    "                    })\n",
    "        self.log_koef = 50\n",
    "        \n",
    "    def step(self, action):\n",
    "\n",
    "        state, reward, done, numstep = self.env1.step(action)\n",
    "        \n",
    "        dict_state = {'img':     state.img,\n",
    "                      'posRobot':state.posRobot,\n",
    "                      'target':  state.target}\n",
    "        \n",
    "#         dist = np.sqrt((dict_state['target'][0]-dict_state['posRobot'][0])**2 + (dict_state['target'][1]-dict_state['posRobot'][1])**2)\n",
    "        \n",
    "#         Ax = np.cos(dict_state['target'][2])\n",
    "#         Ay = np.sin(dict_state['target'][2])\n",
    "#         Bx = dict_state['target'][0] - dict_state['posRobot'][0]\n",
    "#         By = dict_state['target'][1] - dict_state['posRobot'][1]\n",
    "\n",
    "#         phy = np.arccos((Ax*Bx + Ay*By)/(np.sqrt(Ax**2 + Ay**2) * np.sqrt(Bx**2 + By**2)))\n",
    "        \n",
    "#         reward = reward + np.log2(phy/dist*self.log_koef )\n",
    "        \n",
    "        if numstep >= self.steps_limit:\n",
    "            done = True\n",
    "    \n",
    "        return dict_state, reward, done, {}\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        state = self.env1.reset()\n",
    "        \n",
    "        dict_state = {'img':     state.img,  \n",
    "                      'posRobot':state.posRobot,  \n",
    "                      'target':  state.target}  \n",
    "        \n",
    "        return dict_state  \n",
    "\n",
    "    def render(self, model, num_gifs=1):\n",
    "            for i in range(num_gifs):\n",
    "                images = []\n",
    "                obs = self.reset()\n",
    "                img = obs['img']# \n",
    "                done = False\n",
    "                while not done:\n",
    "                    images.append(img)\n",
    "                    action, _ = model.predict(obs)\n",
    "                    obs, _, done ,_ = self.step(action)\n",
    "                    img = obs['img'] #env.render(mode='rgb_array')\n",
    "\n",
    "                imageio.mimsave(f\"video{i}.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import gym\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Dict, features_dim: int = 2310):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        \n",
    "        extractors = {}\n",
    "        \n",
    "        for key, subspace in observation_space.spaces.items():\n",
    "            if key == \"img\":\n",
    "        \n",
    "                n_input_channels = observation_space[key].shape[0]\n",
    "            \n",
    "                extractors[key] = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(n_input_channels, 32, 2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                nn.Conv2d(32, 64, 2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(4, 4),\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                ResBlock(n_filters=64, kernel_size=2), \n",
    "                nn.MaxPool2d(2, 2),\n",
    "                \n",
    "                nn.Conv2d(64, 128, 2),\n",
    "                nn.Flatten())\n",
    "                    \n",
    "            elif key == \"posRobot\":\n",
    "                            \n",
    "                extractors[key] = nn.Sequential(nn.Linear(3, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 3))\n",
    "                    \n",
    "            elif key == \"target\":\n",
    "                            \n",
    "                extractors[key] = nn.Sequential(nn.Linear(3, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 3))\n",
    "                \n",
    "        self.extractors = nn.ModuleDict(extractors)\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        encoded_tensor_list = []\n",
    "\n",
    "        # self.extractors contain nn.Modules that do all the processing.\n",
    "        for key, extractor in self.extractors.items():\n",
    "            encoded_tensor_list.append(extractor(observations[key]))\n",
    "        # Return a (B, self._features_dim) PyTorch tensor, where B is batch dimension.\n",
    "        return th.cat(encoded_tensor_list, dim=1)\n",
    "\n",
    "\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_filters, kernel_size):\n",
    "        super().__init__()\n",
    "        self.n_filters = n_filters # the number of filters represents the number of output channels AND the number of input channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.b1 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "        \n",
    "        self.b2 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99) # in keras the default epsilon = 0.001 & momentum = 0.99\n",
    "        # whereas in putorch the default epsilon = 1e-05 & momentom = 0.1, the output is the same shape of input (N,C,H,W).\n",
    "        self.b3 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "        self.b4 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        y = F.relu(self.b1(x))\n",
    "        y = self.b2(y)\n",
    "        y = F.relu(self.b3(y))\n",
    "        y = self.b4(y)\n",
    "        y += residual\n",
    "        y = F.relu(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, agent_name: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model_' + agent_name)\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim = 518)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = CustomEnv(obstacle_turn = False, \n",
    "                vizualaze     = False, \n",
    "                Total_war     = True,\n",
    "                head_velocity = 0.07, \n",
    "                steps_limit   = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN  # \n",
    "\n",
    "log_dir = './saved_models_disc_mult/DQN/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq  = 1000, \n",
    "                                            log_dir     = log_dir,\n",
    "                                            agent_name  = 'DQN')\n",
    "\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "model = DQN(policy                  = 'MlpPolicy',\n",
    "            env                     = env,\n",
    "            learning_rate           = 0.0001,\n",
    "            buffer_size             = 10000,\n",
    "            batch_size              = 20,\n",
    "            gamma                   = 0.99,\n",
    "            tensorboard_log         = \"./tensorboard_logs_disc_mult/\",\n",
    "            policy_kwargs           = policy_kwargs,\n",
    "            verbose                 = 1,\n",
    "            device                  = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./tensorboard_logs_disc_mult/DQN_1\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -inf - Last mean reward per episode: 59.55\n",
      "Saving new best model to ./saved_models_disc_mult/DQN/best_model_DQN\n",
      "Num timesteps: 3000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -151.61\n",
      "Num timesteps: 4000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -151.61\n",
      "Num timesteps: 5000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -131.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.43e+03 |\n",
      "|    ep_rew_mean      | -289     |\n",
      "|    exploration rate | 0.946    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 156      |\n",
      "|    total timesteps  | 5717     |\n",
      "----------------------------------\n",
      "Num timesteps: 6000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -289.23\n",
      "Num timesteps: 7000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -289.23\n",
      "Num timesteps: 8000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -192.10\n",
      "Num timesteps: 9000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -182.23\n",
      "Num timesteps: 10000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -182.23\n",
      "Num timesteps: 11000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -198.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.38e+03 |\n",
      "|    ep_rew_mean      | -179     |\n",
      "|    exploration rate | 0.895    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 299      |\n",
      "|    total timesteps  | 11042    |\n",
      "----------------------------------\n",
      "Num timesteps: 12000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -179.31\n",
      "Num timesteps: 13000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -162.93\n",
      "Num timesteps: 14000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -163.73\n",
      "Num timesteps: 15000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -163.73\n",
      "Num timesteps: 16000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -160.01\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.38e+03 |\n",
      "|    ep_rew_mean      | -189     |\n",
      "|    exploration rate | 0.842    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 450      |\n",
      "|    total timesteps  | 16602    |\n",
      "----------------------------------\n",
      "Num timesteps: 17000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -188.59\n",
      "Num timesteps: 18000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -188.59\n",
      "Num timesteps: 19000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -201.63\n",
      "Num timesteps: 20000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -160.43\n",
      "Num timesteps: 21000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -160.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.36e+03 |\n",
      "|    ep_rew_mean      | -144     |\n",
      "|    exploration rate | 0.794    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 588      |\n",
      "|    total timesteps  | 21699    |\n",
      "----------------------------------\n",
      "Num timesteps: 22000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -144.35\n",
      "Num timesteps: 23000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -144.35\n",
      "Num timesteps: 24000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -132.06\n",
      "Num timesteps: 25000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -139.80\n",
      "Num timesteps: 26000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -139.80\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.33e+03 |\n",
      "|    ep_rew_mean      | -130     |\n",
      "|    exploration rate | 0.748    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 719      |\n",
      "|    total timesteps  | 26515    |\n",
      "----------------------------------\n",
      "Num timesteps: 27000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -129.65\n",
      "Num timesteps: 28000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -129.65\n",
      "Num timesteps: 29000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -138.47\n",
      "Num timesteps: 30000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -145.59\n",
      "Num timesteps: 31000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -145.59\n",
      "Num timesteps: 32000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -148.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.35e+03 |\n",
      "|    ep_rew_mean      | -149     |\n",
      "|    exploration rate | 0.691    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 881      |\n",
      "|    total timesteps  | 32515    |\n",
      "----------------------------------\n",
      "Num timesteps: 33000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -149.15\n",
      "Num timesteps: 34000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -158.54\n",
      "Num timesteps: 35000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -163.39\n",
      "Num timesteps: 36000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -146.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.29e+03 |\n",
      "|    ep_rew_mean      | -160     |\n",
      "|    exploration rate | 0.656    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 983      |\n",
      "|    total timesteps  | 36240    |\n",
      "----------------------------------\n",
      "Num timesteps: 37000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -160.45\n",
      "Num timesteps: 38000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -161.90\n",
      "Num timesteps: 39000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -161.90\n",
      "Num timesteps: 40000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -164.10\n",
      "Num timesteps: 41000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -163.46\n",
      "Num timesteps: 42000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -163.46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.32e+03 |\n",
      "|    ep_rew_mean      | -166     |\n",
      "|    exploration rate | 0.599    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 1143     |\n",
      "|    total timesteps  | 42240    |\n",
      "----------------------------------\n",
      "Num timesteps: 43000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -153.12\n",
      "Num timesteps: 44000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -153.12\n",
      "Num timesteps: 45000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -153.26\n",
      "Num timesteps: 46000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -145.89\n",
      "Num timesteps: 47000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -145.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.31e+03 |\n",
      "|    ep_rew_mean      | -144     |\n",
      "|    exploration rate | 0.551    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 1277     |\n",
      "|    total timesteps  | 47212    |\n",
      "----------------------------------\n",
      "Num timesteps: 48000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -155.56\n",
      "Num timesteps: 49000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -166.68\n",
      "Num timesteps: 50000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -166.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/user/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:439: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/aten/src/ATen/native/Convolution.cpp:660.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.26e+03 |\n",
      "|    ep_rew_mean      | -185     |\n",
      "|    exploration rate | 0.521    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 1373     |\n",
      "|    total timesteps  | 50370    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.63     |\n",
      "|    n_updates        | 92       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.15e+03 |\n",
      "|    ep_rew_mean      | -182     |\n",
      "|    exploration rate | 0.519    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 1388     |\n",
      "|    total timesteps  | 50672    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.41     |\n",
      "|    n_updates        | 167      |\n",
      "----------------------------------\n",
      "Num timesteps: 51000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -178.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.07e+03 |\n",
      "|    ep_rew_mean      | -163     |\n",
      "|    exploration rate | 0.514    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 1411     |\n",
      "|    total timesteps  | 51139    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.633    |\n",
      "|    n_updates        | 284      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 995      |\n",
      "|    ep_rew_mean      | -124     |\n",
      "|    exploration rate | 0.509    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 35       |\n",
      "|    time_elapsed     | 1440     |\n",
      "|    total timesteps  | 51721    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 430      |\n",
      "----------------------------------\n",
      "Num timesteps: 52000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -107.92\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 937      |\n",
      "|    ep_rew_mean      | -77.8    |\n",
      "|    exploration rate | 0.501    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 35       |\n",
      "|    time_elapsed     | 1479     |\n",
      "|    total timesteps  | 52494    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.669    |\n",
      "|    n_updates        | 623      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 883      |\n",
      "|    ep_rew_mean      | -53      |\n",
      "|    exploration rate | 0.497    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 35       |\n",
      "|    time_elapsed     | 1502     |\n",
      "|    total timesteps  | 52965    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.43     |\n",
      "|    n_updates        | 741      |\n",
      "----------------------------------\n",
      "Num timesteps: 53000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -52.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 840      |\n",
      "|    ep_rew_mean      | -31.7    |\n",
      "|    exploration rate | 0.49     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 34       |\n",
      "|    time_elapsed     | 1540     |\n",
      "|    total timesteps  | 53733    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.543    |\n",
      "|    n_updates        | 933      |\n",
      "----------------------------------\n",
      "Num timesteps: 54000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -20.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 797      |\n",
      "|    ep_rew_mean      | -21      |\n",
      "|    exploration rate | 0.485    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 34       |\n",
      "|    time_elapsed     | 1564     |\n",
      "|    total timesteps  | 54205    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.988    |\n",
      "|    n_updates        | 1051     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 763      |\n",
      "|    ep_rew_mean      | -15      |\n",
      "|    exploration rate | 0.478    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 34       |\n",
      "|    time_elapsed     | 1600     |\n",
      "|    total timesteps  | 54931    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.83     |\n",
      "|    n_updates        | 1232     |\n",
      "----------------------------------\n",
      "Num timesteps: 55000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: -15.05\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 730      |\n",
      "|    ep_rew_mean      | 9.68     |\n",
      "|    exploration rate | 0.473    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 34       |\n",
      "|    time_elapsed     | 1627     |\n",
      "|    total timesteps  | 55486    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.701    |\n",
      "|    n_updates        | 1371     |\n",
      "----------------------------------\n",
      "Num timesteps: 56000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: 23.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 702      |\n",
      "|    ep_rew_mean      | 31.6     |\n",
      "|    exploration rate | 0.467    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 33       |\n",
      "|    time_elapsed     | 1660     |\n",
      "|    total timesteps  | 56149    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 1537     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 678      |\n",
      "|    ep_rew_mean      | 52.2     |\n",
      "|    exploration rate | 0.459    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 33       |\n",
      "|    time_elapsed     | 1699     |\n",
      "|    total timesteps  | 56961    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.44     |\n",
      "|    n_updates        | 1740     |\n",
      "----------------------------------\n",
      "Num timesteps: 57000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: 52.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 652      |\n",
      "|    ep_rew_mean      | 61.4     |\n",
      "|    exploration rate | 0.455    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 33       |\n",
      "|    time_elapsed     | 1720     |\n",
      "|    total timesteps  | 57374    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.872    |\n",
      "|    n_updates        | 1843     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 630      |\n",
      "|    ep_rew_mean      | 80.3     |\n",
      "|    exploration rate | 0.45     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 33       |\n",
      "|    time_elapsed     | 1747     |\n",
      "|    total timesteps  | 57933    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.281    |\n",
      "|    n_updates        | 1983     |\n",
      "----------------------------------\n",
      "Num timesteps: 58000\n",
      "Best mean reward: 59.55 - Last mean reward per episode: 80.33\n",
      "Saving new best model to ./saved_models_disc_mult/DQN/best_model_DQN\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 608      |\n",
      "|    ep_rew_mean      | 82.2     |\n",
      "|    exploration rate | 0.446    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 32       |\n",
      "|    time_elapsed     | 1768     |\n",
      "|    total timesteps  | 58349    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.486    |\n",
      "|    n_updates        | 2087     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 589      |\n",
      "|    ep_rew_mean      | 90.6     |\n",
      "|    exploration rate | 0.441    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 32       |\n",
      "|    time_elapsed     | 1794     |\n",
      "|    total timesteps  | 58881    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.67     |\n",
      "|    n_updates        | 2220     |\n",
      "----------------------------------\n",
      "Num timesteps: 59000\n",
      "Best mean reward: 80.33 - Last mean reward per episode: 97.71\n",
      "Saving new best model to ./saved_models_disc_mult/DQN/best_model_DQN\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 535      |\n",
      "|    ep_rew_mean      | 116      |\n",
      "|    exploration rate | 0.438    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 32       |\n",
      "|    time_elapsed     | 1810     |\n",
      "|    total timesteps  | 59183    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.55     |\n",
      "|    n_updates        | 2295     |\n",
      "----------------------------------\n",
      "Num timesteps: 60000\n",
      "Best mean reward: 97.71 - Last mean reward per episode: 123.25\n",
      "Saving new best model to ./saved_models_disc_mult/DQN/best_model_DQN\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 490      |\n",
      "|    ep_rew_mean      | 126      |\n",
      "|    exploration rate | 0.43     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 32       |\n",
      "|    time_elapsed     | 1852     |\n",
      "|    total timesteps  | 60042    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.28     |\n",
      "|    n_updates        | 2510     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 440      |\n",
      "|    ep_rew_mean      | 153      |\n",
      "|    exploration rate | 0.424    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 32       |\n",
      "|    time_elapsed     | 1881     |\n",
      "|    total timesteps  | 60649    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.71     |\n",
      "|    n_updates        | 2662     |\n",
      "----------------------------------\n",
      "Num timesteps: 61000\n",
      "Best mean reward: 123.25 - Last mean reward per episode: 167.72\n",
      "Saving new best model to ./saved_models_disc_mult/DQN/best_model_DQN\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 394      |\n",
      "|    ep_rew_mean      | 175      |\n",
      "|    exploration rate | 0.419    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 32       |\n",
      "|    time_elapsed     | 1905     |\n",
      "|    total timesteps  | 61130    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.17     |\n",
      "|    n_updates        | 2782     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 350      |\n",
      "|    ep_rew_mean      | 196      |\n",
      "|    exploration rate | 0.415    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 31       |\n",
      "|    time_elapsed     | 1925     |\n",
      "|    total timesteps  | 61527    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.31     |\n",
      "|    n_updates        | 2881     |\n",
      "----------------------------------\n",
      "Num timesteps: 62000\n",
      "Best mean reward: 167.72 - Last mean reward per episode: 221.87\n",
      "Saving new best model to ./saved_models_disc_mult/DQN/best_model_DQN\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 296      |\n",
      "|    ep_rew_mean      | 229      |\n",
      "|    exploration rate | 0.41     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 31       |\n",
      "|    time_elapsed     | 1956     |\n",
      "|    total timesteps  | 62146    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.26     |\n",
      "|    n_updates        | 3036     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 263      |\n",
      "|    ep_rew_mean      | 256      |\n",
      "|    exploration rate | 0.406    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 31       |\n",
      "|    time_elapsed     | 1974     |\n",
      "|    total timesteps  | 62512    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.6      |\n",
      "|    n_updates        | 3127     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 206      |\n",
      "|    ep_rew_mean      | 278      |\n",
      "|    exploration rate | 0.403    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 31       |\n",
      "|    time_elapsed     | 1991     |\n",
      "|    total timesteps  | 62836    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.27     |\n",
      "|    n_updates        | 3208     |\n",
      "----------------------------------\n",
      "Num timesteps: 63000\n",
      "Best mean reward: 221.87 - Last mean reward per episode: 280.38\n",
      "Saving new best model to ./saved_models_disc_mult/DQN/best_model_DQN\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 162      |\n",
      "|    ep_rew_mean      | 288      |\n",
      "|    exploration rate | 0.398    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 31       |\n",
      "|    time_elapsed     | 2019     |\n",
      "|    total timesteps  | 63402    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.07     |\n",
      "|    n_updates        | 3350     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 136      |\n",
      "|    ep_rew_mean      | 336      |\n",
      "|    exploration rate | 0.392    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 31       |\n",
      "|    time_elapsed     | 2048     |\n",
      "|    total timesteps  | 63974    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.83     |\n",
      "|    n_updates        | 3493     |\n",
      "----------------------------------\n",
      "Num timesteps: 64000\n",
      "Best mean reward: 280.38 - Last mean reward per episode: 336.09\n",
      "Saving new best model to ./saved_models_disc_mult/DQN/best_model_DQN\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 137      |\n",
      "|    ep_rew_mean      | 362      |\n",
      "|    exploration rate | 0.389    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 31       |\n",
      "|    time_elapsed     | 2065     |\n",
      "|    total timesteps  | 64324    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.33     |\n",
      "|    n_updates        | 3580     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 138      |\n",
      "|    ep_rew_mean      | 379      |\n",
      "|    exploration rate | 0.383    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 30       |\n",
      "|    time_elapsed     | 2096     |\n",
      "|    total timesteps  | 64940    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.35     |\n",
      "|    n_updates        | 3734     |\n",
      "----------------------------------\n",
      "Num timesteps: 65000\n",
      "Best mean reward: 336.09 - Last mean reward per episode: 378.84\n",
      "Saving new best model to ./saved_models_disc_mult/DQN/best_model_DQN\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 135      |\n",
      "|    ep_rew_mean      | 382      |\n",
      "|    exploration rate | 0.38     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 30       |\n",
      "|    time_elapsed     | 2113     |\n",
      "|    total timesteps  | 65268    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.873    |\n",
      "|    n_updates        | 3816     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 132      |\n",
      "|    ep_rew_mean      | 380      |\n",
      "|    exploration rate | 0.376    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 30       |\n",
      "|    time_elapsed     | 2134     |\n",
      "|    total timesteps  | 65693    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.28     |\n",
      "|    n_updates        | 3923     |\n",
      "----------------------------------\n",
      "Num timesteps: 66000\n",
      "Best mean reward: 378.84 - Last mean reward per episode: 381.01\n",
      "Saving new best model to ./saved_models_disc_mult/DQN/best_model_DQN\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 130      |\n",
      "|    ep_rew_mean      | 382      |\n",
      "|    exploration rate | 0.373    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 30       |\n",
      "|    time_elapsed     | 2150     |\n",
      "|    total timesteps  | 66010    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.2      |\n",
      "|    n_updates        | 4002     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 126      |\n",
      "|    ep_rew_mean      | 385      |\n",
      "|    exploration rate | 0.37     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 30       |\n",
      "|    time_elapsed     | 2167     |\n",
      "|    total timesteps  | 66349    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.77     |\n",
      "|    n_updates        | 4087     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 125      |\n",
      "|    ep_rew_mean      | 398      |\n",
      "|    exploration rate | 0.367    |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 30       |\n",
      "|    time_elapsed     | 2183     |\n",
      "|    total timesteps  | 66674    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.826    |\n",
      "|    n_updates        | 4168     |\n",
      "----------------------------------\n",
      "Num timesteps: 67000\n",
      "Best mean reward: 381.01 - Last mean reward per episode: 418.63\n",
      "Saving new best model to ./saved_models_disc_mult/DQN/best_model_DQN\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 122      |\n",
      "|    ep_rew_mean      | 418      |\n",
      "|    exploration rate | 0.363    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 30       |\n",
      "|    time_elapsed     | 2204     |\n",
      "|    total timesteps  | 67101    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.3      |\n",
      "|    n_updates        | 4275     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 119      |\n",
      "|    ep_rew_mean      | 415      |\n",
      "|    exploration rate | 0.36     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 30       |\n",
      "|    time_elapsed     | 2219     |\n",
      "|    total timesteps  | 67396    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.865    |\n",
      "|    n_updates        | 4348     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 115      |\n",
      "|    ep_rew_mean      | 410      |\n",
      "|    exploration rate | 0.357    |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 30       |\n",
      "|    time_elapsed     | 2233     |\n",
      "|    total timesteps  | 67684    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.32     |\n",
      "|    n_updates        | 4420     |\n",
      "----------------------------------\n",
      "Num timesteps: 68000\n",
      "Best mean reward: 418.63 - Last mean reward per episode: 409.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 110      |\n",
      "|    ep_rew_mean      | 408      |\n",
      "|    exploration rate | 0.354    |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 30       |\n",
      "|    time_elapsed     | 2250     |\n",
      "|    total timesteps  | 68010    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.781    |\n",
      "|    n_updates        | 4502     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 110      |\n",
      "|    ep_rew_mean      | 419      |\n",
      "|    exploration rate | 0.35     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 30       |\n",
      "|    time_elapsed     | 2269     |\n",
      "|    total timesteps  | 68406    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.714    |\n",
      "|    n_updates        | 4601     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 108      |\n",
      "|    ep_rew_mean      | 415      |\n",
      "|    exploration rate | 0.347    |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 30       |\n",
      "|    time_elapsed     | 2285     |\n",
      "|    total timesteps  | 68709    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.705    |\n",
      "|    n_updates        | 4677     |\n",
      "----------------------------------\n",
      "Num timesteps: 69000\n",
      "Best mean reward: 418.63 - Last mean reward per episode: 427.17\n",
      "Saving new best model to ./saved_models_disc_mult/DQN/best_model_DQN\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 108      |\n",
      "|    ep_rew_mean      | 425      |\n",
      "|    exploration rate | 0.343    |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2307     |\n",
      "|    total timesteps  | 69163    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 4790     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | 431      |\n",
      "|    exploration rate | 0.34     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2324     |\n",
      "|    total timesteps  | 69515    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.814    |\n",
      "|    n_updates        | 4878     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 107      |\n",
      "|    ep_rew_mean      | 434      |\n",
      "|    exploration rate | 0.336    |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2341     |\n",
      "|    total timesteps  | 69843    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.819    |\n",
      "|    n_updates        | 4960     |\n",
      "----------------------------------\n",
      "Num timesteps: 70000\n",
      "Best mean reward: 427.17 - Last mean reward per episode: 435.90\n",
      "Saving new best model to ./saved_models_disc_mult/DQN/best_model_DQN\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 440      |\n",
      "|    exploration rate | 0.333    |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2359     |\n",
      "|    total timesteps  | 70191    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.32     |\n",
      "|    n_updates        | 5047     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.2     |\n",
      "|    ep_rew_mean      | 437      |\n",
      "|    exploration rate | 0.33     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2373     |\n",
      "|    total timesteps  | 70474    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.615    |\n",
      "|    n_updates        | 5118     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 96.4     |\n",
      "|    ep_rew_mean      | 432      |\n",
      "|    exploration rate | 0.328    |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2388     |\n",
      "|    total timesteps  | 70770    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.868    |\n",
      "|    n_updates        | 5192     |\n",
      "----------------------------------\n",
      "Num timesteps: 71000\n",
      "Best mean reward: 435.90 - Last mean reward per episode: 431.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.4     |\n",
      "|    ep_rew_mean      | 432      |\n",
      "|    exploration rate | 0.325    |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2403     |\n",
      "|    total timesteps  | 71068    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.22     |\n",
      "|    n_updates        | 5266     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 93.2     |\n",
      "|    ep_rew_mean      | 425      |\n",
      "|    exploration rate | 0.321    |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2422     |\n",
      "|    total timesteps  | 71466    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.587    |\n",
      "|    n_updates        | 5366     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.7     |\n",
      "|    ep_rew_mean      | 426      |\n",
      "|    exploration rate | 0.318    |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2438     |\n",
      "|    total timesteps  | 71782    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.533    |\n",
      "|    n_updates        | 5445     |\n",
      "----------------------------------\n",
      "Num timesteps: 72000\n",
      "Best mean reward: 435.90 - Last mean reward per episode: 427.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.6     |\n",
      "|    ep_rew_mean      | 428      |\n",
      "|    exploration rate | 0.315    |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2454     |\n",
      "|    total timesteps  | 72094    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.16     |\n",
      "|    n_updates        | 5523     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.2     |\n",
      "|    ep_rew_mean      | 434      |\n",
      "|    exploration rate | 0.313    |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2465     |\n",
      "|    total timesteps  | 72323    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.955    |\n",
      "|    n_updates        | 5580     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 86.7     |\n",
      "|    ep_rew_mean      | 429      |\n",
      "|    exploration rate | 0.31     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2481     |\n",
      "|    total timesteps  | 72647    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.518    |\n",
      "|    n_updates        | 5661     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 86.5     |\n",
      "|    ep_rew_mean      | 422      |\n",
      "|    exploration rate | 0.307    |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2498     |\n",
      "|    total timesteps  | 72979    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.937    |\n",
      "|    n_updates        | 5744     |\n",
      "----------------------------------\n",
      "Num timesteps: 73000\n",
      "Best mean reward: 435.90 - Last mean reward per episode: 421.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 83.5     |\n",
      "|    ep_rew_mean      | 423      |\n",
      "|    exploration rate | 0.304    |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2514     |\n",
      "|    total timesteps  | 73291    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.99     |\n",
      "|    n_updates        | 5822     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 83.1     |\n",
      "|    ep_rew_mean      | 418      |\n",
      "|    exploration rate | 0.301    |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2528     |\n",
      "|    total timesteps  | 73577    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.817    |\n",
      "|    n_updates        | 5894     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 82.1     |\n",
      "|    ep_rew_mean      | 414      |\n",
      "|    exploration rate | 0.298    |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 29       |\n",
      "|    time_elapsed     | 2545     |\n",
      "|    total timesteps  | 73907    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.04     |\n",
      "|    n_updates        | 5976     |\n",
      "----------------------------------\n",
      "Num timesteps: 74000\n",
      "Best mean reward: 435.90 - Last mean reward per episode: 414.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 81.3     |\n",
      "|    ep_rew_mean      | 414      |\n",
      "|    exploration rate | 0.296    |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 2556     |\n",
      "|    total timesteps  | 74143    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.29     |\n",
      "|    n_updates        | 6035     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 81.3     |\n",
      "|    ep_rew_mean      | 416      |\n",
      "|    exploration rate | 0.292    |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 2573     |\n",
      "|    total timesteps  | 74479    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.76     |\n",
      "|    n_updates        | 6119     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 80.4     |\n",
      "|    ep_rew_mean      | 410      |\n",
      "|    exploration rate | 0.29     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 2585     |\n",
      "|    total timesteps  | 74716    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.808    |\n",
      "|    n_updates        | 6178     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 78.8     |\n",
      "|    ep_rew_mean      | 402      |\n",
      "|    exploration rate | 0.288    |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 2599     |\n",
      "|    total timesteps  | 74985    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.98     |\n",
      "|    n_updates        | 6246     |\n",
      "----------------------------------\n",
      "Num timesteps: 75000\n",
      "Best mean reward: 435.90 - Last mean reward per episode: 401.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 78.9     |\n",
      "|    ep_rew_mean      | 404      |\n",
      "|    exploration rate | 0.285    |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 2614     |\n",
      "|    total timesteps  | 75284    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.793    |\n",
      "|    n_updates        | 6320     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 78.1     |\n",
      "|    ep_rew_mean      | 405      |\n",
      "|    exploration rate | 0.283    |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 2624     |\n",
      "|    total timesteps  | 75493    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.95     |\n",
      "|    n_updates        | 6373     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 78.3     |\n",
      "|    ep_rew_mean      | 410      |\n",
      "|    exploration rate | 0.28     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 2642     |\n",
      "|    total timesteps  | 75836    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.05     |\n",
      "|    n_updates        | 6458     |\n",
      "----------------------------------\n",
      "Num timesteps: 76000\n",
      "Best mean reward: 435.90 - Last mean reward per episode: 412.14\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 78.2     |\n",
      "|    ep_rew_mean      | 416      |\n",
      "|    exploration rate | 0.276    |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 2661     |\n",
      "|    total timesteps  | 76222    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.782    |\n",
      "|    n_updates        | 6555     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 78.3     |\n",
      "|    ep_rew_mean      | 421      |\n",
      "|    exploration rate | 0.273    |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 2677     |\n",
      "|    total timesteps  | 76541    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.498    |\n",
      "|    n_updates        | 6635     |\n",
      "----------------------------------\n",
      "Num timesteps: 77000\n",
      "Best mean reward: 435.90 - Last mean reward per episode: 419.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 79.2     |\n",
      "|    ep_rew_mean      | 426      |\n",
      "|    exploration rate | 0.268    |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 2704     |\n",
      "|    total timesteps  | 77086    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.321    |\n",
      "|    n_updates        | 6771     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 79       |\n",
      "|    ep_rew_mean      | 421      |\n",
      "|    exploration rate | 0.265    |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 2721     |\n",
      "|    total timesteps  | 77411    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.01     |\n",
      "|    n_updates        | 6852     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 78.6     |\n",
      "|    ep_rew_mean      | 418      |\n",
      "|    exploration rate | 0.262    |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 2735     |\n",
      "|    total timesteps  | 77706    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.37     |\n",
      "|    n_updates        | 6926     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 77.3     |\n",
      "|    ep_rew_mean      | 416      |\n",
      "|    exploration rate | 0.26     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 2746     |\n",
      "|    total timesteps  | 77919    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.71     |\n",
      "|    n_updates        | 6979     |\n",
      "----------------------------------\n",
      "Num timesteps: 78000\n",
      "Best mean reward: 435.90 - Last mean reward per episode: 415.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 76.9     |\n",
      "|    ep_rew_mean      | 417      |\n",
      "|    exploration rate | 0.257    |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 28       |\n",
      "|    time_elapsed     | 2758     |\n",
      "|    total timesteps  | 78160    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.723    |\n",
      "|    n_updates        | 7039     |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1e6,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render(model,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir ./tensorboard_logs_disc_mult/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PPO.load(\"./saved_models_disc_mult/TD3/best_model_PPo1\", env=env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
