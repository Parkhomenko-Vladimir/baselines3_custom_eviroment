{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from Enviroment import Enviroment\n",
    "from gym import spaces\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Callback class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model_PPO')\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.rew_len = []\n",
    "        self.n_games = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \n",
    "        if self.n_games < len(model.ep_info_buffer):\n",
    "            self.n_games+=1\n",
    "            self.rew_len.append(model.ep_info_buffer[-1]['r'])\n",
    "            \n",
    "            if self.best_mean_reward < np.mean(self.rew_len[-100:]):\n",
    "                self.best_mean_reward = np.mean(self.rew_len[-100:])\n",
    "                self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Dict, features_dim: int = 518):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        \n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(n_input_channels, 32, 2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            ResBlock(n_filters=64, kernel_size=2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            ResBlock(n_filters=64, kernel_size=2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            ResBlock(n_filters=64, kernel_size=2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            ResBlock(n_filters=64, kernel_size=2), \n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                th.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "\n",
    "        print(n_flatten)\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "        \n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        '''\n",
    "        Forward propagation\n",
    "        :param observations: (dict) изображение; координаты и углы ориентации агентов\n",
    "        :return: features tensor\n",
    "        '''\n",
    "\n",
    "        return self.linear(self.cnn(observations)) \n",
    "\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_filters, kernel_size):\n",
    "        \"\"\"\n",
    "        Инициализация кастомного резнетовского блока\n",
    "        :param n_filters: (int) количество фильтров сверточного слоя\n",
    "        :param kernel_size: (int) размер ядра свертки\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.b1 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "    \n",
    "        self.b2 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99)\n",
    "        self.b3 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "        self.b4 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward propagation\n",
    "        :param x: input\n",
    "        :return: output\n",
    "        '''\n",
    "        residual = x\n",
    "        y = F.relu(self.b1(x))\n",
    "        y = self.b2(y)\n",
    "        y = F.relu(self.b3(y))\n",
    "        y = self.b4(y)\n",
    "        y += residual\n",
    "        y = F.relu(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment gym class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import cv2\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatAction(gym.Wrapper):\n",
    "    def __init__(self, env=None, repeat=4, fire_first=False):\n",
    "        super(RepeatAction, self).__init__(env)\n",
    "        self.repeat = repeat\n",
    "        self.shape = env.observation_space.low.shape\n",
    "        self.fire_first = fire_first\n",
    "\n",
    "    def step(self, action):\n",
    "        t_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self.repeat):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            t_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, t_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        if self.fire_first:\n",
    "            assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "            obs, _,_,_ = self.env.step(1)\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, shape, env=None):\n",
    "        super(PreprocessFrame, self).__init__(env)\n",
    "        self.shape = (shape[2], shape[0], shape[1])\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n",
    "                                                shape=self.shape,\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "        resized_screen = cv2.resize(new_frame, self.shape[1:],\n",
    "                                    interpolation=cv2.INTER_AREA)\n",
    "        new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)\n",
    "        new_obs = new_obs / 255.0\n",
    "\n",
    "        return new_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackFrames(gym.ObservationWrapper):\n",
    "    def __init__(self,env , repeat):\n",
    "        super(StackFrames, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            env.observation_space.low.repeat(repeat, axis=0),\n",
    "            env.observation_space.high.repeat(repeat, axis=0),\n",
    "            dtype=np.float32)\n",
    "        self.stack = collections.deque(maxlen=repeat)\n",
    "\n",
    "    def reset(self):\n",
    "        self.stack.clear()\n",
    "        observation = self.env.reset()\n",
    "        for _ in range(self.stack.maxlen):\n",
    "            self.stack.append(observation)\n",
    "\n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.stack.append(observation)\n",
    "\n",
    "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    '''\n",
    "    Оборочивание класса среды в среду gym\n",
    "    '''\n",
    "\n",
    "    def __init__(self, obstacle_turn: bool, Total_war: bool, num_obs: int, num_enemy: int, inp_dim: int,\n",
    "                 size_obs, steps_limit, vizualaze=False, head_velocity=0.01,\n",
    "                rew_col = -100,rew_win=100, rew_defeat = -100):\n",
    "        '''\n",
    "        Инициализация класса среды\n",
    "        :param obstacle_turn: (bool) Флаг генерации препятствий\n",
    "        :param vizualaze: (bool) Флаг генерации препятствий\n",
    "        :param Total_war: (bool) Флаг режима игры (с противником или без)\n",
    "        :param steps_limit: (int) Максимальное количество действий в среде за одну игру\n",
    "        '''\n",
    "        self.log_koef = 50\n",
    "\n",
    "        self.velocity_coef = 35       #  1/2 max speed !!!\n",
    "        self.ang_Norm_coef = 1\n",
    "        self.coords_Norm_coef = 500\n",
    "        self.proportional_coef = 0.01\n",
    "        self.inp_dim = inp_dim\n",
    "        \n",
    "        self.rew_col = rew_col\n",
    "        self.rew_win = rew_win\n",
    "        self.rew_defeat = rew_defeat\n",
    "                \n",
    "        self.enviroment = Enviroment(obstacle_turn, vizualaze, Total_war,\n",
    "                                     head_velocity, num_obs, num_enemy, size_obs, steps_limit,\n",
    "                                     rew_col, rew_win, rew_defeat,epsilon = 100,sigma = 30)\n",
    "\n",
    "        self.action_space = spaces.Box(low=np.array([-1, -1]), high=np.array([1, 1]), dtype=np.float16) # [speed, angle]\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(self.inp_dim, self.inp_dim, 3), dtype=np.uint8)        \n",
    "        \n",
    "        state = self.enviroment.reset()\n",
    "        \n",
    "   \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Метод осуществления шага в среде\n",
    "        :param action: (int) направление движения в среде\n",
    "        :return: dict_state, reward, not done, {}: состояние, реворд, флаг терминального состояния, информация о среде\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        action[1] *= self.ang_Norm_coef\n",
    "        action[0] = self.velocity_coef * np.clip(action[0], a_min = 0.001, a_max = 1)\n",
    "        \n",
    "        state, reward, done, numstep = self.enviroment.step(action)\n",
    "        \n",
    "    \n",
    "        x2 = state.posRobot[0]\n",
    "        y2 = state.posRobot[1]\n",
    "    \n",
    "        x4 = state.target[0,0]\n",
    "        y4 = state.target[0,1]\n",
    "        \n",
    "        f2 =  state.target[0,2]\n",
    "        \n",
    "        Ax4, Ay4 = -np.cos(f2), np.sin(f2)\n",
    "        Bx24, By24 = x2 - x4, y2 - y4\n",
    "        \n",
    "        dist = - np.sqrt(np.abs((x2-x4)**2 + (y2-y4)**2))\n",
    "        phy = (Ax4*Bx24 + Ay4*By24)/(np.sqrt(Ax4**2 + Ay4**2) * np.sqrt(Bx24**2 + By24**2))\n",
    "        reward_l = phy*(dist+500) * self.proportional_coef * (not done) + np.round(reward, 2).sum()\n",
    "\n",
    "        return state.img, reward_l, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Метод обновления игры\n",
    "        :return: dict_state: состояние\n",
    "        '''\n",
    "        \n",
    "        state = self.enviroment.reset()\n",
    "        \n",
    "        dict_state = state.img \n",
    "\n",
    "\n",
    "        return dict_state\n",
    "\n",
    "    def render(self, model, num_gifs=1):\n",
    "        '''\n",
    "        Метод вывода информации об игре\n",
    "        :param mode:\n",
    "        :return:\n",
    "        '''\n",
    "        for i in range(num_gifs):\n",
    "            \n",
    "            images = []\n",
    "            obs = self.reset()\n",
    "            img = obs['img']# env.render(mode='rgb_array')\n",
    "            done = False\n",
    "                \n",
    "            height, width, layers = img.shape\n",
    "            size = (width,height)\n",
    "            out = cv2.VideoWriter(f\"video{i}.avi\",cv2.VideoWriter_fourcc(*'DIVX'), 25, size)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            out.write(img)\n",
    "            while not done:\n",
    "\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, _, done ,_ = self.step(action)\n",
    "                img = obs['img']\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                out.write(img)\n",
    "            out.release()\n",
    "    \n",
    "    def get_statistic(self, model, num_games):\n",
    "        collision = 0\n",
    "        win = 0\n",
    "        destroyed = 0\n",
    "        loss = 0\n",
    "        \n",
    "        pbar = tqdm(range(num_games))\n",
    "        for i in pbar:\n",
    "            obs = self.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, reward, done ,_ = self.step(action)\n",
    "                \n",
    "            if reward == self.rew_col:      # collision\n",
    "                collision+=1\n",
    "            elif reward == self.rew_win:    # win\n",
    "                win +=1\n",
    "            elif reward == self.rew_defeat: # loss\n",
    "                destroyed +=1\n",
    "            else:                           # not_achieved\n",
    "                loss+=1\n",
    "        \n",
    "        print(\"Win: \",win/num_games)\n",
    "        print(\"destroyed: \", destroyed/num_games)\n",
    "        print(\"loss: \",loss/num_games)\n",
    "        print(\"collision: \",collision/num_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv(obstacle_turn = True,\n",
    "                vizualaze     = True, \n",
    "                Total_war     = True,\n",
    "                head_velocity = 0.005,#\n",
    "                num_obs       = 5, \n",
    "                num_enemy     = 1, \n",
    "                size_obs      = [50, 60],\n",
    "                rew_col       = -70,\n",
    "                rew_win       = 100,\n",
    "                inp_dim       = 250,#!!!!!!!!!!!!\n",
    "                rew_defeat    = -100,\n",
    "                steps_limit   = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RepeatAction(env, repeat = 4)\n",
    "env = PreprocessFrame( shape=(250,250,1), env = env)\n",
    "env = StackFrames(env, repeat = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.step([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.step(np.array([4, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, TD3, DDPG, SAC\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=134),\n",
    "    activation_fn=torch.nn.ReLU,\n",
    "    net_arch = [dict(pi=[134, 32, 8], vf=[134, 32, 8])])\n",
    "\n",
    "model = PPO(policy          = 'MlpPolicy',\n",
    "            env             = env,\n",
    "            learning_rate   = 0.001,\n",
    "            n_steps         = 2048,\n",
    "            batch_size      = 72,\n",
    "            gamma           = 0.99,\n",
    "            gae_lambda      = 0.95,\n",
    "            tensorboard_log = \"./tensorboard_logs/\",\n",
    "            policy_kwargs   = policy_kwargs,\n",
    "            verbose         = 1,\n",
    "            device          = 'cuda')\n",
    "\n",
    "# model = A2C(policy          = 'MlpPolicy',\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             n_steps         = 24,\n",
    "#             gamma           = 0.99,\n",
    "#             gae_lambda      = 0.95,\n",
    "#             tensorboard_log = \"./tensorboard_logs/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 1,\n",
    "#             device          = 'cuda')\n",
    "# model = TD3(policy          = 'MlpPolicy',  # 1 neural network metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             buffer_size     = 100,\n",
    "#             batch_size      = 2,\n",
    "#             gamma           = 0.99,\n",
    "#             tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')\n",
    "\n",
    "# model = DDPG(policy         = 'MlpPolicy',  # 1 neural network metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             buffer_size     = 100,\n",
    "#             batch_size      = 2,\n",
    "#             gamma           = 0.99,\n",
    "#             tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')\n",
    "\n",
    "# model = SAC(policy          = 'MlpPolicy',  # 1 neural network metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             buffer_size     = 100,\n",
    "#             batch_size      = 2,\n",
    "#             gamma           = 0.99,\n",
    "#             tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = './saved_models/PPO'  # For A2C agent: './saved_models/A2C'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=500, \n",
    "                                            log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1e6,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make environment to test trained model and get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_test = CustomEnv(obstacle_turn = False,\n",
    "                    vizualaze      = False, \n",
    "                    Total_war      = True,\n",
    "                    head_velocity  = 0.005,\n",
    "                    num_obs        = 5, \n",
    "                    num_enemy      = 1, \n",
    "                    size_obs       = [30, 40],\n",
    "                    rew_col        = -100,\n",
    "                    rew_win        = 100,\n",
    "                    rew_defeat     = -100,\n",
    "                    steps_limit    = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best model and get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './saved_models/PPO/best_model_PPO'  # For A2C agent: './saved_models/A2C/callback0/best_model_A2C/'\n",
    "model = PPO.load(path, env=env_test,)  # For A2C agent: A2C.load(path, env=env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_test.get_statistic(model, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_test.render(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir ./tensorboard_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
