{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from Enviroment import Enviroment\n",
    "from gym import spaces\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Callback class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model_PPO')\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.rew_len = []\n",
    "        self.n_games = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \n",
    "        if self.n_games < len(model.ep_info_buffer):\n",
    "            self.n_games+=1\n",
    "            self.rew_len.append(model.ep_info_buffer[-1]['r'])\n",
    "            \n",
    "            if self.best_mean_reward < np.mean(self.rew_len[-100:]):\n",
    "                self.best_mean_reward = np.mean(self.rew_len[-100:])\n",
    "                self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Dict, features_dim: int = 518):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        \n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(n_input_channels, 32, 2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            ResBlock(n_filters=64, kernel_size=2),\n",
    "            # nn.MaxPool2d(4, 4),\n",
    "            # ResBlock(n_filters=64, kernel_size=2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            ResBlock(n_filters=64, kernel_size=2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            ResBlock(n_filters=64, kernel_size=2), \n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                th.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "\n",
    "        print(n_flatten)\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "        \n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        '''\n",
    "        Forward propagation\n",
    "        :param observations: (dict) изображение; координаты и углы ориентации агентов\n",
    "        :return: features tensor\n",
    "        '''\n",
    "\n",
    "        return self.linear(self.cnn(observations)) \n",
    "\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_filters, kernel_size):\n",
    "        \"\"\"\n",
    "        Инициализация кастомного резнетовского блока\n",
    "        :param n_filters: (int) количество фильтров сверточного слоя\n",
    "        :param kernel_size: (int) размер ядра свертки\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.b1 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "    \n",
    "        self.b2 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99)\n",
    "        self.b3 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "        self.b4 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward propagation\n",
    "        :param x: input\n",
    "        :return: output\n",
    "        '''\n",
    "        residual = x\n",
    "        y = F.relu(self.b1(x))\n",
    "        y = self.b2(y)\n",
    "        y = F.relu(self.b3(y))\n",
    "        y = self.b4(y)\n",
    "        y += residual\n",
    "        y = F.relu(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment gym class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    '''\n",
    "    Оборочивание класса среды в среду gym\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, obstacle_turn: bool, Total_war: bool, num_obs: int, num_enemy: int, \n",
    "                 size_obs, steps_limit, vizualaze=False, head_velocity=0.01,\n",
    "                rew_col = -100,rew_win=100, rew_defeat = -100):\n",
    "        '''\n",
    "        Инициализация класса среды\n",
    "        :param obstacle_turn: (bool) Флаг генерации препятствий\n",
    "        :param vizualaze: (bool) Флаг генерации препятствий\n",
    "        :param Total_war: (bool) Флаг режима игры (с противником или без)\n",
    "        :param steps_limit: (int) Максимальное количество действий в среде за одну игру\n",
    "        '''\n",
    "        self.log_koef = 50\n",
    "\n",
    "        self.velocity_coef = 35       #  1/2 max speed !!!\n",
    "        self.ang_Norm_coef = np.pi\n",
    "        self.coords_Norm_coef = 500\n",
    "        self.proportional_coef = 0.01\n",
    "        self.imd_dim = 100\n",
    "        \n",
    "        self.rew_col = rew_col\n",
    "        self.rew_win = rew_win\n",
    "        self.rew_defeat = rew_defeat\n",
    "                \n",
    "        self.enviroment = Enviroment(obstacle_turn, vizualaze, Total_war,\n",
    "                                     head_velocity, num_obs, num_enemy, size_obs, steps_limit,\n",
    "                                     rew_col, rew_win, rew_defeat,epsilon = 100,sigma = 30)\n",
    "\n",
    "        self.action_space = spaces.Box(low=np.array([-1, -1]), high=np.array([1, 1]), dtype=np.float16)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(self.imd_dim, self.imd_dim, 3), dtype=np.uint8)        \n",
    "        \n",
    "        state = self.enviroment.reset()\n",
    "        state.img = cv2.resize(state.img, (self.imd_dim,self.imd_dim))\n",
    "        \n",
    "        self.img1 = state.img\n",
    "        self.img2 = state.img \n",
    "        self.img3 = state.img \n",
    "        self.Img = None\n",
    "\n",
    "    def make_layers(self):\n",
    "        \"\"\"\n",
    "        Функция наслоения изображений трех последовательных шагов в среде\n",
    "        :param img1, img2, img3: состояния среды на трех последовательных шагах\n",
    "        :return: new_img: изображение, содержащее информацию о состояниях среды на трех последовательных шагах, отображенную с разной интенсивностью\n",
    "        \"\"\"\n",
    "        new_img = cv2.addWeighted(self.img2, 0.4, self.img1, 0.2, 0)\n",
    "        self.Img = cv2.addWeighted(self.img3, 0.7, new_img, 0.5, 0)\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Метод осуществления шага в среде\n",
    "        :param action: (int) направление движения в среде\n",
    "        :return: dict_state, reward, not done, {}: состояние, реворд, флаг терминального состояния, информация о среде\n",
    "        \"\"\"\n",
    "        \n",
    "        action[0] = self.velocity_coef/3 * (action[0] + 1) + self.velocity_coef \n",
    "        action[1] *= self.ang_Norm_coef\n",
    "        \n",
    "        state, reward, done, numstep = self.enviroment.step(action)\n",
    "        state.img = cv2.resize(state.img, (self.imd_dim,self.imd_dim))\n",
    "        \n",
    "        self.img1 = self.img2\n",
    "        self.img2 = self.img3\n",
    "        self.img3 = state.img\n",
    "        \n",
    "        self.make_layers()\n",
    "    \n",
    "        x2 = state.posRobot[0]\n",
    "        y2 = state.posRobot[1]\n",
    "    \n",
    "        x4 = state.target[0,0]\n",
    "        y4 = state.target[0,1]\n",
    "        \n",
    "        f2 =  state.target[0,2]\n",
    "        \n",
    "        Ax4, Ay4 = -np.cos(f2), np.sin(f2)\n",
    "        Bx24, By24 = x2 - x4, y2 - y4\n",
    "        \n",
    "        dist = - np.sqrt(np.abs((x2-x4)**2 + (y2-y4)**2))\n",
    "        phy = (Ax4*Bx24 + Ay4*By24)/(np.sqrt(Ax4**2 + Ay4**2) * np.sqrt(Bx24**2 + By24**2))\n",
    "        reward_l = phy*(dist+500) * self.proportional_coef * (not done) + np.round(reward, 2).sum()\n",
    "\n",
    "        return self.Img, reward_l, done, {}\n",
    "    \n",
    "    def normTarget(self, coords):\n",
    "        '''\n",
    "        Метод нормализации координат\n",
    "        :return: coords: нормализованные координаты\n",
    "        '''\n",
    "        coords=np.float32(coords)\n",
    "        coords[:,2]  = coords[:,2] / self.ang_Norm_coef #угол\n",
    "        coords[:,:2] = coords[:,:2] / self.coords_Norm_coef #координаты\n",
    "        \n",
    "        return coords\n",
    "\n",
    "    def normPoseRobot(self, coords):\n",
    "        '''\n",
    "        Метод нормализации координат\n",
    "        :return: coords: нормализованные координаты\n",
    "        '''\n",
    "        coords=np.float32(coords)\n",
    "        coords[2]  = coords[2] / self.ang_Norm_coef #угол\n",
    "        coords[:2] = coords[:2] / self.coords_Norm_coef #координаты\n",
    "        \n",
    "        return coords\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Метод обновления игры\n",
    "        :return: dict_state: состояние\n",
    "        '''\n",
    "        \n",
    "        state = self.enviroment.reset()\n",
    "        state.img = cv2.resize(state.img, (self.imd_dim,self.imd_dim))\n",
    "        \n",
    "        self.img2 = state.img\n",
    "        self.img3 = state.img\n",
    "        \n",
    "        dict_state = state.img \n",
    "\n",
    "\n",
    "        return dict_state\n",
    "\n",
    "    def render(self, model, num_gifs=1):\n",
    "        '''\n",
    "        Метод вывода информации об игре\n",
    "        :param mode:\n",
    "        :return:\n",
    "        '''\n",
    "        for i in range(num_gifs):\n",
    "            \n",
    "            images = []\n",
    "            obs = self.reset()\n",
    "            img = obs['img']# env.render(mode='rgb_array')\n",
    "            done = False\n",
    "                \n",
    "            height, width, layers = img.shape\n",
    "            size = (width,height)\n",
    "            out = cv2.VideoWriter(f\"video{i}.avi\",cv2.VideoWriter_fourcc(*'DIVX'), 25, size)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            out.write(img)\n",
    "            while not done:\n",
    "\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, _, done ,_ = self.step(action)\n",
    "                img = obs['img']\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                out.write(img)\n",
    "            out.release()\n",
    "    \n",
    "    def get_statistic(self, model, num_games):\n",
    "        collision = 0\n",
    "        win = 0\n",
    "        destroyed = 0\n",
    "        loss = 0\n",
    "        \n",
    "        pbar = tqdm(range(num_games))\n",
    "        for i in pbar:\n",
    "            obs = self.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, reward, done ,_ = self.step(action)\n",
    "                \n",
    "            if reward == self.rew_col:      # collision\n",
    "                collision+=1\n",
    "            elif reward == self.rew_win:    # win\n",
    "                win +=1\n",
    "            elif reward == self.rew_defeat: # loss\n",
    "                destroyed +=1\n",
    "            else:                           # not_achieved\n",
    "                loss+=1\n",
    "        \n",
    "        print(\"Win: \",win/num_games)\n",
    "        print(\"destroyed: \", destroyed/num_games)\n",
    "        print(\"loss: \",loss/num_games)\n",
    "        print(\"collision: \",collision/num_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv(obstacle_turn = True,\n",
    "                vizualaze     = False, \n",
    "                Total_war     = True,\n",
    "                head_velocity = 0.005,#\n",
    "                num_obs       = 5, \n",
    "                num_enemy     = 1, \n",
    "                size_obs      = [50, 60],\n",
    "                rew_col       = -70,\n",
    "                rew_win       = 100,\n",
    "                rew_defeat    = -100,\n",
    "                steps_limit   = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step([1,\n",
    "                                      np.pi/100])\n",
    "# print(np.round(reward,2).sum())\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, TD3, DDPG, SAC\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=134),\n",
    "    activation_fn=torch.nn.ReLU,\n",
    "    net_arch = [dict(pi=[134, 32, 8], vf=[134, 32, 8])])\n",
    "\n",
    "model = PPO(policy          = 'MlpPolicy',\n",
    "            env             = env,\n",
    "            learning_rate   = 0.0001,\n",
    "            n_steps         = 2048,\n",
    "            batch_size      = 24,\n",
    "            gamma           = 0.99,\n",
    "            gae_lambda      = 0.95,\n",
    "            tensorboard_log = \"./tensorboard_logs/\",\n",
    "            policy_kwargs   = policy_kwargs,\n",
    "            verbose         = 1,\n",
    "            device          = 'cuda')\n",
    "\n",
    "# model = A2C(policy          = 'MlpPolicy',\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             n_steps         = 24,\n",
    "#             gamma           = 0.99,\n",
    "#             gae_lambda      = 0.95,\n",
    "#             tensorboard_log = \"./tensorboard_logs/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 1,\n",
    "#             device          = 'cuda')\n",
    "# model = TD3(policy          = 'MlpPolicy',  # 1 neural network metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             buffer_size     = 100,\n",
    "#             batch_size      = 2,\n",
    "#             gamma           = 0.99,\n",
    "#             tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')\n",
    "\n",
    "# model = DDPG(policy         = 'MlpPolicy',  # 1 neural network metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             buffer_size     = 100,\n",
    "#             batch_size      = 2,\n",
    "#             gamma           = 0.99,\n",
    "#             tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')\n",
    "\n",
    "# model = SAC(policy          = 'MlpPolicy',  # 1 neural network metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             buffer_size     = 100,\n",
    "#             batch_size      = 2,\n",
    "#             gamma           = 0.99,\n",
    "#             tensorboard_log = \"./tensorboard_logs_cont_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = './saved_models/PPO'  # For A2C agent: './saved_models/A2C'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=500, \n",
    "                                            log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1e6,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make environment to test trained model and get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = CustomEnv(obstacle_turn = False,\n",
    "                    vizualaze      = False, \n",
    "                    Total_war      = True,\n",
    "                    head_velocity  = 0.005,\n",
    "                    num_obs        = 5, \n",
    "                    num_enemy      = 1, \n",
    "                    size_obs       = [30, 40],\n",
    "                    rew_col        = -100,\n",
    "                    rew_win        = 100,\n",
    "                    rew_defeat     = -100,\n",
    "                    steps_limit    = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best model and get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './saved_models/PPO/best_model_PPO'  # For A2C agent: './saved_models/A2C/callback0/best_model_A2C/'\n",
    "model = PPO.load(path, env=env_test,)  # For A2C agent: A2C.load(path, env=env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_test.get_statistic(model, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_test.render(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir ./tensorboard_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
