{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from Enviroment import Enviroment\n",
    "from gym import spaces\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Callback class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model_PPO')\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.rew_len = []\n",
    "        self.n_games = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \n",
    "        if self.n_games < len(model.ep_info_buffer):\n",
    "            self.n_games+=1\n",
    "            self.rew_len.append(model.ep_info_buffer[-1]['r'])\n",
    "            \n",
    "            if self.best_mean_reward < np.mean(self.rew_len[-100:]):\n",
    "                self.best_mean_reward = np.mean(self.rew_len[-100:])\n",
    "                self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Dict, features_dim: int = 518):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        \n",
    "        extractors = {}\n",
    "        \n",
    "        for key, subspace in observation_space.spaces.items():\n",
    "            if key == \"img\":\n",
    "        \n",
    "                n_input_channels = observation_space[key].shape[0]\n",
    "            \n",
    "                extractors[key] = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(n_input_channels, 32, 2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                nn.Conv2d(32, 64, 2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(4, 4),\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                ResBlock(n_filters=64, kernel_size=2), \n",
    "                nn.MaxPool2d(2, 2),\n",
    "                \n",
    "                nn.Conv2d(64, 128, 2),\n",
    "                nn.Flatten())\n",
    "                    \n",
    "            elif key == \"posRobot\":\n",
    "                \n",
    "                n_input_channels = observation_space[key].shape[0]\n",
    "                \n",
    "                extractors[key] = nn.Sequential(nn.Linear(n_input_channels, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 3))\n",
    "                    \n",
    "            elif key == \"target\":\n",
    "                            \n",
    "                n_input_channels = observation_space[key].shape[0]\n",
    "                    \n",
    "                extractors[key] = nn.Sequential(nn.Linear(n_input_channels, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 3))\n",
    "                \n",
    "        self.extractors = nn.ModuleDict(extractors)\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        '''\n",
    "        Forward propagation\n",
    "        :param observations: (dict) изображение; координаты и углы ориентации агентов\n",
    "        :return: features tensor\n",
    "        '''\n",
    "        encoded_tensor_list = []\n",
    "\n",
    "        for key, extractor in self.extractors.items():\n",
    "            encoded_tensor_list.append(extractor(observations[key]))\n",
    "\n",
    "        return th.cat(encoded_tensor_list, dim=1)\n",
    "\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_filters, kernel_size):\n",
    "        \"\"\"\n",
    "        Инициализация кастомного резнетовского блока\n",
    "        :param n_filters: (int) количество фильтров сверточного слоя\n",
    "        :param kernel_size: (int) размер ядра свертки\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.b1 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "    \n",
    "        self.b2 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99)\n",
    "        self.b3 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "        self.b4 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward propagation\n",
    "        :param x: input\n",
    "        :return: output\n",
    "        '''\n",
    "        residual = x\n",
    "        y = F.relu(self.b1(x))\n",
    "        y = self.b2(y)\n",
    "        y = F.relu(self.b3(y))\n",
    "        y = self.b4(y)\n",
    "        y += residual\n",
    "        y = F.relu(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Environment gym class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    '''\n",
    "    Оборочивание класса среды в среду gym\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, obstacle_turn: bool, Total_war: bool, num_obs, num_enemy: int, inp_dim: int,\n",
    "                 size_obs, steps_limit, vizualaze = False, head_velocity = 0.01, rew_col = -70,\n",
    "                 rew_win = 100, rew_defeat = -100 , EnemyLidSet = [120, 120] , AllyLidSet = [50, 90]):\n",
    "        '''\n",
    "        Инициализация класса среды\n",
    "        :param obstacle_turn: (bool) Флаг генерации препятствий\n",
    "        :param vizualaze: (bool) Флаг генерации препятствий\n",
    "        :param Total_war: (bool) Флаг режима игры (с противником или без)\n",
    "        :param steps_limit: (int) Максимальное количество действий в среде за одну игру\n",
    "        '''\n",
    "        self.log_koef = 50\n",
    "        self.ang_Norm_coef = np.pi\n",
    "        self.coords_Norm_coef = 500\n",
    "        self.inp_dim = inp_dim\n",
    "        \n",
    "        # optionEnemy = [120, 120]     # настройки противника [0] - дальность СТЗ; [1] - угол СТЗ (градусы)\n",
    "        # optionAlie = [50, 90]        # настройки союзника [0] - дальность СТЗ; [1] - угол СТЗ (градусы)\n",
    "        \n",
    "        self.obstacle_turn = obstacle_turn\n",
    "        self.Total_war = Total_war\n",
    "        self.num_obs = num_obs\n",
    "        self.num_enemy = num_enemy\n",
    "        self.size_obs = size_obs\n",
    "        self.steps_limit = steps_limit\n",
    "        self.vizualaze = vizualaze\n",
    "        self.head_velocity = head_velocity\n",
    "        self.rew_col = rew_col\n",
    "        self.rew_win = rew_win\n",
    "        self.rew_defeat = rew_defeat\n",
    "        self.EnemyLidSet = EnemyLidSet\n",
    "        self.AllyLidSet = AllyLidSet\n",
    "        \n",
    "        self.enviroment = Enviroment(self.obstacle_turn, self.vizualaze, self.Total_war,\n",
    "                                     self.head_velocity, self.num_obs, self.num_enemy, \n",
    "                                     self.size_obs, self.steps_limit, self.rew_col, \n",
    "                                     self.rew_win, self.rew_defeat, epsilon = 100,\n",
    "                                     sigma = 30, optionEnemy = self.EnemyLidSet, optionAlie = self.AllyLidSet)\n",
    "\n",
    "\n",
    "        self.action_space = spaces.Discrete(8)\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "                    'img': spaces.Box(low=0, high=255, shape=(self.inp_dim, self.inp_dim, 3), dtype=np.uint8),\n",
    "                    'posRobot': spaces.Box(low=np.array([0, 0,-3.14]), high=np.array([500, 500, 3.14])),\n",
    "                    'target':   spaces.Box(low=np.array([0, 0,-3.14]), high = np.array([500, 500, 3.14]))\n",
    "                                                })\n",
    "\n",
    "        state = self.enviroment.reset()\n",
    "        \n",
    "        self.img1 = state.img\n",
    "        self.img2 = state.img\n",
    "        self.img3 = state.img\n",
    "        self.Img = None\n",
    "        \n",
    "\n",
    "    def make_layers(self):\n",
    "        \"\"\"\n",
    "        Функция наслоения изображений трех последовательных шагов в среде\n",
    "        :param img1, img2, img3: состояния среды на трех последовательных шагах\n",
    "        :return: new_img: изображение, содержащее информацию о состояниях среды на трех последовательных шагах, отображенную с разной интенсивностью\n",
    "        \"\"\"\n",
    "        new_img = cv2.addWeighted(self.img2, 0.4, self.img1, 0.2, 0)\n",
    "        self.Img = cv2.addWeighted(self.img3, 0.7, new_img, 0.5, 0)\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Метод осуществления шага в среде\n",
    "        :param action: (int) направление движения в среде\n",
    "        :return: dict_state, reward, not done, {}: состояние, реворд, флаг терминального состояния, информация о среде\n",
    "        \"\"\"\n",
    "        \n",
    "        state, reward, done, numstep = self.enviroment.step(action)\n",
    "        # state.img = cv2.resize(state.img, (self.inp_dim,self.inp_dim))\n",
    "        \n",
    "        self.img1 = self.img2\n",
    "        self.img2 = self.img3\n",
    "        self.img3 = state.img\n",
    "        \n",
    "        self.make_layers()\n",
    "    \n",
    "        x2 = state.posRobot[0]\n",
    "        y2 = state.posRobot[1]\n",
    "    \n",
    "        x4 = state.target[0,0]\n",
    "        y4 = state.target[0,1]\n",
    "        \n",
    "        \n",
    "        f2 =  state.target[0,2]\n",
    "        f2 = np.deg2rad(f2)\n",
    "        \n",
    "        Ax4, Ay4 = -np.cos(f2), np.sin(f2)\n",
    "        Bx24, By24 = x2 - x4, y2 - y4\n",
    "\n",
    "        dist = - np.sqrt(np.abs((x2-x4)**2 + (y2-y4)**2))\n",
    "        phy = (Ax4*Bx24 + Ay4*By24)/(np.sqrt(Ax4**2 + Ay4**2) * np.sqrt(Bx24**2 + By24**2))\n",
    "        reward_l = phy*(dist+500) * 0.01 * (not done) + np.round(reward, 2).sum()\n",
    "\n",
    "        \n",
    "        dict_state = {'img':     self.Img,  \n",
    "                      'posRobot':self.normPoseRobot(state.posRobot),  \n",
    "                      'target':  self.normTarget(state.target).reshape(-1)}\n",
    "\n",
    "        return dict_state, reward_l, done, {}\n",
    "    \n",
    "    def normTarget(self, coords):\n",
    "        '''\n",
    "        Метод нормализации координат\n",
    "        :return: coords: нормализованные координаты\n",
    "        '''\n",
    "        coords=np.float32(coords)\n",
    "        coords[:,2]  = coords[:,2] / self.ang_Norm_coef #угол\n",
    "        coords[:,:2] = coords[:,:2] / self.coords_Norm_coef #координаты\n",
    "        \n",
    "        return coords\n",
    "\n",
    "    def normPoseRobot(self, coords):\n",
    "        '''\n",
    "        Метод нормализации координат\n",
    "        :return: coords: нормализованные координаты\n",
    "        '''\n",
    "        coords=np.float32(coords)\n",
    "        coords[2]  = coords[2] / self.ang_Norm_coef #угол\n",
    "        coords[:2] = coords[:2] / self.coords_Norm_coef #координаты\n",
    "        \n",
    "        return coords\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Метод обновления игры\n",
    "        :return: dict_state: состояние\n",
    "        '''\n",
    "                \n",
    "        state = self.enviroment.reset()\n",
    "        # state.img = cv2.resize(state.img, (self.inp_dim,self.inp_dim))\n",
    "        \n",
    "        self.img2 = state.img\n",
    "        self.img3 = state.img\n",
    "        \n",
    "        dict_state = {'img':     state.img,  \n",
    "                      'posRobot':self.normPoseRobot(state.posRobot),  \n",
    "                      'target':  self.normTarget(state.target).reshape(-1)}\n",
    "\n",
    "        return dict_state\n",
    "\n",
    "    def render(self, model, num_gifs=1):\n",
    "        '''\n",
    "        Метод вывода информации об игре\n",
    "        :param mode:\n",
    "        :return:\n",
    "        '''\n",
    "        for i in range(num_gifs):\n",
    "            \n",
    "            images = []\n",
    "            obs = self.reset()\n",
    "            img = obs['img']# env.render(mode='rgb_array')\n",
    "            done = False\n",
    "                \n",
    "            height, width, layers = img.shape\n",
    "            size = (width,height)\n",
    "            out = cv2.VideoWriter(f\"video{i}.avi\",cv2.VideoWriter_fourcc(*'DIVX'), 25, size)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            out.write(img)\n",
    "            while not done:\n",
    "\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, _, done ,_ = self.step(action)\n",
    "                img = obs['img']\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                out.write(img)\n",
    "            out.release()\n",
    "    \n",
    "    def get_statistic(self, model, num_games):\n",
    "        collision = 0\n",
    "        win = 0\n",
    "        destroyed = 0\n",
    "        loss = 0\n",
    "        \n",
    "        pbar = tqdm(range(num_games))\n",
    "        for i in pbar:\n",
    "            obs = self.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, reward, done ,_ = self.step(action)\n",
    "                \n",
    "            if reward == self.rew_col:#win\n",
    "                collision+=1\n",
    "            elif reward == self.rew_win:# loss\n",
    "                win +=1\n",
    "            elif reward == self.rew_defeat:# loss\n",
    "                destroyed +=1\n",
    "            else:    #not_achieved\n",
    "                loss+=1\n",
    "        \n",
    "        print(\"Win: \",win/num_games)\n",
    "        print(\"destroyed: \", destroyed/num_games)\n",
    "        print(\"loss: \",loss/num_games)\n",
    "        print(\"collision: \",collision/num_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv(obstacle_turn = True,\n",
    "                vizualaze     = False, \n",
    "                Total_war     = True,\n",
    "                inp_dim       = 500,\n",
    "                head_velocity = 0.005,#0.005\n",
    "                num_obs       = 5, \n",
    "                num_enemy     = 1, \n",
    "                size_obs      = [50, 60],\n",
    "                rew_col       = -70,\n",
    "                rew_win       = 100,\n",
    "                rew_defeat    = -100,\n",
    "                steps_limit   = 2000,\n",
    "                EnemyLidSet   = [45,90],\n",
    "                AllyLidSet    = [40, 90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialize the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=512+6),\n",
    "    activation_fn=torch.nn.ReLU,\n",
    "    net_arch = [dict(pi=[134, 32, 8], vf=[134, 32, 8])])\n",
    "\n",
    "# model = DQN(policy                  = 'MultiInputPolicy',  # 1 neural network metod\n",
    "#             env                     = env,\n",
    "#             learning_rate           = 0.0001,\n",
    "#             buffer_size             = 10000,\n",
    "#             batch_size              = 20,\n",
    "#             gamma                   = 0.99,\n",
    "#             tensorboard_log         = \"./tensorboard_logs_disc_mult/\",\n",
    "#             policy_kwargs           = policy_kwargs,\n",
    "#             verbose                 = 0,\n",
    "#             device                  = 'cuda')\n",
    "\n",
    "model = PPO(policy          = 'MultiInputPolicy',  # 2 neural networks metod\n",
    "            env             = env,\n",
    "            learning_rate   = 0.001,\n",
    "            n_steps         = 64,\n",
    "            batch_size      = 64,\n",
    "            gamma           = 0.99,\n",
    "            gae_lambda      = 0.95,\n",
    "            tensorboard_log = \"./tensorboard_logs/\",\n",
    "            policy_kwargs   = policy_kwargs,\n",
    "            verbose         = 1,\n",
    "            device          = 'cuda')\n",
    "\n",
    "# model = A2C(policy          = 'MlpPolicy',  # 2 neural networks metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             n_steps         = 10,\n",
    "#             gamma           = 0.99,\n",
    "#             gae_lambda      = 0.95,\n",
    "#             tensorboard_log = \"./tensorboard_logs_disc_mult/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = './saved_models/PPO/callback/'  # For A2C agent: './saved_models/A2C/callback'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=5000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=5e5,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best model and get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = './saved_models/PPO/callback0/best_model_PPO/'  # For A2C agent: './saved_models/A2C/callback0/best_model_A2C/'\n",
    "# model = PPO.load(path, env=env_test,device = 'cuda')  # For A2C agent: A2C.load(path, env=env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# env.get_statistic(model, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir ./tensorboard_logs_discr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
