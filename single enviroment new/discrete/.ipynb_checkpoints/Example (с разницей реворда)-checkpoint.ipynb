{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.9.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from Enviroment import Enviroment\n",
    "from gym import spaces\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Callback class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model_PPO')\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.rew_len = []\n",
    "        self.n_games = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \n",
    "        if self.n_games < len(model.ep_info_buffer):\n",
    "            self.n_games+=1\n",
    "            self.rew_len.append(model.ep_info_buffer[-1]['r'])\n",
    "            \n",
    "            if self.best_mean_reward < np.mean(self.rew_len[-100:]):\n",
    "                self.best_mean_reward = np.mean(self.rew_len[-100:])\n",
    "                self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Dict, features_dim: int = 518):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        \n",
    "        extractors = {}\n",
    "        \n",
    "        for key, subspace in observation_space.spaces.items():\n",
    "            if key == \"img\":\n",
    "        \n",
    "                n_input_channels = observation_space[key].shape[0]\n",
    "            \n",
    "                extractors[key] = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(n_input_channels, 32, 2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                nn.Conv2d(32, 64, 2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(4, 4),\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                ResBlock(n_filters=64, kernel_size=2),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                ResBlock(n_filters=64, kernel_size=2), \n",
    "                nn.MaxPool2d(2, 2),\n",
    "                \n",
    "                nn.Conv2d(64, 128, 2),\n",
    "                nn.Flatten())\n",
    "                    \n",
    "            elif key == \"posRobot\":\n",
    "                \n",
    "                n_input_channels = observation_space[key].shape[0]\n",
    "                \n",
    "                extractors[key] = nn.Sequential(nn.Linear(n_input_channels, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 3))\n",
    "                    \n",
    "            elif key == \"target\":\n",
    "                            \n",
    "                n_input_channels = observation_space[key].shape[0]\n",
    "                    \n",
    "                extractors[key] = nn.Sequential(nn.Linear(n_input_channels, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 9),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(9, 3))\n",
    "                \n",
    "        self.extractors = nn.ModuleDict(extractors)\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        '''\n",
    "        Forward propagation\n",
    "        :param observations: (dict) изображение; координаты и углы ориентации агентов\n",
    "        :return: features tensor\n",
    "        '''\n",
    "        encoded_tensor_list = []\n",
    "\n",
    "        for key, extractor in self.extractors.items():\n",
    "            encoded_tensor_list.append(extractor(observations[key]))\n",
    "\n",
    "        return th.cat(encoded_tensor_list, dim=1)\n",
    "\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_filters, kernel_size):\n",
    "        \"\"\"\n",
    "        Инициализация кастомного резнетовского блока\n",
    "        :param n_filters: (int) количество фильтров сверточного слоя\n",
    "        :param kernel_size: (int) размер ядра свертки\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.b1 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "    \n",
    "        self.b2 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99)\n",
    "        self.b3 = nn.Conv2d(self.n_filters, self.n_filters, self.kernel_size, padding='same')\n",
    "        self.b4 = nn.BatchNorm2d(self.n_filters, eps = 0.001, momentum= 0.99)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward propagation\n",
    "        :param x: input\n",
    "        :return: output\n",
    "        '''\n",
    "        residual = x\n",
    "        y = F.relu(self.b1(x))\n",
    "        y = self.b2(y)\n",
    "        y = F.relu(self.b3(y))\n",
    "        y = self.b4(y)\n",
    "        y += residual\n",
    "        y = F.relu(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Environment gym class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    '''\n",
    "    Оборочивание класса среды в среду gym\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, obstacle_turn: bool, Total_war: bool, num_obs, num_enemy: int, inp_dim: int,\n",
    "                 size_obs, steps_limit, vizualaze = False, head_velocity = 0.01, rew_col = -70,\n",
    "                 rew_win = 100, rew_defeat = -100 , EnemyLidSet = [120, 120] , AllyLidSet = [50, 90]):\n",
    "        '''\n",
    "        Инициализация класса среды\n",
    "        :param obstacle_turn: (bool) Флаг генерации препятствий\n",
    "        :param vizualaze: (bool) Флаг генерации препятствий\n",
    "        :param Total_war: (bool) Флаг режима игры (с противником или без)\n",
    "        :param steps_limit: (int) Максимальное количество действий в среде за одну игру\n",
    "        '''\n",
    "        self.log_koef = 50\n",
    "        self.ang_Norm_coef = np.pi\n",
    "        self.coords_Norm_coef = 500\n",
    "        self.inp_dim = inp_dim\n",
    "        \n",
    "        # optionEnemy = [120, 120]     # настройки противника [0] - дальность СТЗ; [1] - угол СТЗ (градусы)\n",
    "        # optionAlie = [50, 90]        # настройки союзника [0] - дальность СТЗ; [1] - угол СТЗ (градусы)\n",
    "        \n",
    "        self.obstacle_turn = obstacle_turn\n",
    "        self.Total_war = Total_war\n",
    "        self.num_obs = num_obs\n",
    "        self.num_enemy = num_enemy\n",
    "        self.size_obs = size_obs\n",
    "        self.steps_limit = steps_limit\n",
    "        self.vizualaze = vizualaze\n",
    "        self.head_velocity = head_velocity\n",
    "        self.rew_col = rew_col\n",
    "        self.rew_win = rew_win\n",
    "        self.rew_defeat = rew_defeat\n",
    "        self.EnemyLidSet = EnemyLidSet\n",
    "        self.AllyLidSet = AllyLidSet\n",
    "        \n",
    "        self.prew_reward = 0\n",
    "        \n",
    "        self.enviroment = Enviroment(self.obstacle_turn, self.vizualaze, self.Total_war,\n",
    "                                     self.head_velocity, self.num_obs, self.num_enemy, \n",
    "                                     self.size_obs, self.steps_limit, self.rew_col, \n",
    "                                     self.rew_win, self.rew_defeat, epsilon = 100,\n",
    "                                     sigma = 30, optionEnemy = self.EnemyLidSet, optionAlie = self.AllyLidSet)\n",
    "        \n",
    "        self.action_space = spaces.Discrete(8)\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "                    'img': spaces.Box(low=0, high=255, shape=(self.inp_dim, self.inp_dim, 3), dtype=np.uint8),\n",
    "                    'posRobot': spaces.Box(low=np.array([0, 0,-3.14]), high=np.array([500, 500, 3.14])),\n",
    "                    'target':   spaces.Box(low=np.array([0, 0,-3.14]), high = np.array([500, 500, 3.14]))\n",
    "                                                })\n",
    "\n",
    "        state = self.enviroment.reset()\n",
    "        \n",
    "        self.img1 = state.img\n",
    "        self.img2 = state.img\n",
    "        self.img3 = state.img\n",
    "        self.Img = None\n",
    "\n",
    "    def make_layers(self):\n",
    "        \"\"\"\n",
    "        Функция наслоения изображений трех последовательных шагов в среде\n",
    "        :param img1, img2, img3: состояния среды на трех последовательных шагах\n",
    "        :return: new_img: изображение, содержащее информацию о состояниях среды на трех последовательных шагах, отображенную с разной интенсивностью\n",
    "        \"\"\"\n",
    "        new_img = cv2.addWeighted(self.img2, 0.4, self.img1, 0.2, 0)\n",
    "        self.Img = cv2.addWeighted(self.img3, 0.7, new_img, 0.5, 0)\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Метод осуществления шага в среде\n",
    "        :param action: (int) направление движения в среде\n",
    "        :return: dict_state, reward, not done, {}: состояние, реворд, флаг терминального состояния, информация о среде\n",
    "        \"\"\"\n",
    "\n",
    "        state, reward, done, numstep = self.enviroment.step(action)\n",
    "        # state.img = cv2.resize(state.img, (self.inp_dim,self.inp_dim))\n",
    "        \n",
    "        self.img1 = self.img2\n",
    "        self.img2 = self.img3\n",
    "        self.img3 = state.img\n",
    "        \n",
    "        self.make_layers()\n",
    "    \n",
    "        x2 = state.posRobot[0]\n",
    "        y2 = state.posRobot[1]\n",
    "    \n",
    "        x4 = state.target[0,0]\n",
    "        y4 = state.target[0,1]\n",
    "        \n",
    "        f2 =  state.target[0,2] \n",
    "        f2 = np.deg2rad(f2)\n",
    "        \n",
    "        Ax4, Ay4 = -np.cos(f2), np.sin(f2)\n",
    "        Bx24, By24 = x2 - x4, y2 - y4\n",
    "\n",
    "        dist = - np.sqrt(np.abs((x2-x4)**2 + (y2-y4)**2))\n",
    "        phy = (Ax4*Bx24 + Ay4*By24)/(np.sqrt(Ax4**2 + Ay4**2) * np.sqrt(Bx24**2 + By24**2))\n",
    "        reward = phy*(dist+500) * (not done) + np.round(reward, 2).sum()\n",
    "        \n",
    "        difference = reward - self.prew_reward\n",
    "        self.prew_reward = reward\n",
    "        \n",
    "        dict_state = {'img':     self.Img,  \n",
    "                      'posRobot':self.normPoseRobot(state.posRobot),  \n",
    "                      'target':  self.normTarget(state.target).reshape(-1)}\n",
    "\n",
    "        return dict_state, difference, done, {}\n",
    "    \n",
    "    def normTarget(self, coords):\n",
    "        '''\n",
    "        Метод нормализации координат\n",
    "        :return: coords: нормализованные координаты\n",
    "        '''\n",
    "        coords=np.float32(coords)\n",
    "        coords[:,2]  = coords[:,2] / self.ang_Norm_coef #угол\n",
    "        coords[:,:2] = coords[:,:2] / self.coords_Norm_coef #координаты\n",
    "        \n",
    "        return coords\n",
    "\n",
    "    def normPoseRobot(self, coords):\n",
    "        '''\n",
    "        Метод нормализации координат\n",
    "        :return: coords: нормализованные координаты\n",
    "        '''\n",
    "        coords=np.float32(coords)\n",
    "        coords[2]  = coords[2] / self.ang_Norm_coef #угол\n",
    "        coords[:2] = coords[:2] / self.coords_Norm_coef #координаты\n",
    "        \n",
    "        return coords\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Метод обновления игры\n",
    "        :return: dict_state: состояние\n",
    "        '''\n",
    "        \n",
    "        state = self.enviroment.reset()\n",
    "        # state.img = cv2.resize(state.img, (self.inp_dim,self.inp_dim))\n",
    "        \n",
    "        self.prew_reward = 0\n",
    "        \n",
    "        self.img2 = state.img\n",
    "        self.img3 = state.img\n",
    "        \n",
    "        dict_state = {'img':     state.img,  \n",
    "                      'posRobot':self.normPoseRobot(state.posRobot),  \n",
    "                      'target':  self.normTarget(state.target).reshape(-1)}\n",
    "\n",
    "        return dict_state\n",
    "\n",
    "    def render(self, model, num_gifs=1):\n",
    "        '''\n",
    "        Метод вывода информации об игре\n",
    "        :param mode:\n",
    "        :return:\n",
    "        '''\n",
    "        for i in range(num_gifs):\n",
    "            \n",
    "            images = []\n",
    "            obs = self.reset()\n",
    "            img = obs['img']# env.render(mode='rgb_array')\n",
    "            done = False\n",
    "                \n",
    "            height, width, layers = img.shape\n",
    "            size = (width,height)\n",
    "            out = cv2.VideoWriter(f\"video{i}.avi\",cv2.VideoWriter_fourcc(*'DIVX'), 25, size)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            out.write(img)\n",
    "            while not done:\n",
    "\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, _, done ,_ = self.step(action)\n",
    "                img = obs['img']\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                out.write(img)\n",
    "            out.release()\n",
    "    \n",
    "    def get_statistic(self, model, num_games):\n",
    "        collision = 0\n",
    "        win = 0\n",
    "        destroyed = 0\n",
    "        loss = 0\n",
    "        \n",
    "        pbar = tqdm(range(num_games))\n",
    "        for i in pbar:\n",
    "            obs = self.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs)\n",
    "                obs, reward, done ,_ = self.step(action)\n",
    "                \n",
    "            if reward == self.rew_col:#win\n",
    "                collision+=1\n",
    "            elif reward == self.rew_win:# loss\n",
    "                win +=1\n",
    "            elif reward == self.rew_defeat:# loss\n",
    "                destroyed +=1\n",
    "            else:    #not_achieved\n",
    "                loss+=1\n",
    "        \n",
    "        print(\"Win: \",win/num_games)\n",
    "        print(\"destroyed: \", destroyed/num_games)\n",
    "        print(\"loss: \",loss/num_games)\n",
    "        print(\"collision: \",collision/num_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parkhomenko/anaconda3/envs/torch/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = CustomEnv(obstacle_turn = True,\n",
    "                vizualaze     = False, \n",
    "                Total_war     = True,\n",
    "                inp_dim       = 500,\n",
    "                head_velocity = 0.005,#0.005\n",
    "                num_obs       = 5, \n",
    "                num_enemy     = 1, \n",
    "                size_obs      = [50, 60],\n",
    "                rew_col       = -70,\n",
    "                rew_win       = 100,\n",
    "                rew_defeat    = -100,\n",
    "                steps_limit   = 2000,\n",
    "                EnemyLidSet   = [70,100],\n",
    "                AllyLidSet    = [70, 120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=512+6),\n",
    "    activation_fn=torch.nn.ReLU,\n",
    "    net_arch = [dict(pi=[134, 32, 8], vf=[134, 32, 8])])\n",
    "\n",
    "# model = DQN(policy                  = 'MultiInputPolicy',  # 1 neural network metod\n",
    "#             env                     = env,\n",
    "#             learning_rate           = 0.0001,\n",
    "#             buffer_size             = 10000,\n",
    "#             batch_size              = 20,\n",
    "#             gamma                   = 0.99,\n",
    "#             tensorboard_log         = \"./tensorboard_logs_discr/\",\n",
    "#             policy_kwargs           = policy_kwargs,\n",
    "#             verbose                 = 0,\n",
    "#             device                  = 'cuda')\n",
    "\n",
    "model = PPO(policy          = 'MultiInputPolicy',  # 2 neural networks metod\n",
    "            env             = env,\n",
    "            learning_rate   = 0.001,\n",
    "            n_steps         = 248,\n",
    "            batch_size      = 36,\n",
    "            gamma           = 0.99,\n",
    "            gae_lambda      = 0.95,\n",
    "            tensorboard_log = \"./tensorboard_logs_discr/\",\n",
    "            policy_kwargs   = policy_kwargs,\n",
    "            verbose         = 1,\n",
    "            device          = 'cuda')\n",
    "\n",
    "# model = A2C(policy          = 'MlpPolicy',  # 2 neural networks metod\n",
    "#             env             = env,\n",
    "#             learning_rate   = 0.0001,\n",
    "#             n_steps         = 10,\n",
    "#             gamma           = 0.99,\n",
    "#             gae_lambda      = 0.95,\n",
    "#             tensorboard_log = \"./tensorboard_logs_discr/\",\n",
    "#             policy_kwargs   = policy_kwargs,\n",
    "#             verbose         = 0,\n",
    "#             device          = 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = './saved_models/PPO/callback/'  # For A2C agent: './saved_models/A2C/callback'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, log_dir)\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=5000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./tensorboard_logs_discr/PPO_5\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 34  |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 7   |\n",
      "|    total_timesteps | 248 |\n",
      "----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 496         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030066982 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.03       |\n",
      "|    explained_variance   | -0.00944    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 31.4        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0351     |\n",
      "|    value_loss           | 215         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 744         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020880708 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.01       |\n",
      "|    explained_variance   | 0.00992     |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 5.37        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    value_loss           | 79.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 971         |\n",
      "|    ep_rew_mean          | -79.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 992         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023050811 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.03       |\n",
      "|    explained_variance   | 0.0794      |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 15.1        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    value_loss           | 40.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 971         |\n",
      "|    ep_rew_mean          | -79.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 1240        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037256274 |\n",
      "|    clip_fraction        | 0.472       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.95       |\n",
      "|    explained_variance   | -0.344      |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 9.12        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0397     |\n",
      "|    value_loss           | 63.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 482        |\n",
      "|    ep_rew_mean          | -73.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 17         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 84         |\n",
      "|    total_timesteps      | 1488       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03283841 |\n",
      "|    clip_fraction        | 0.373      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.96      |\n",
      "|    explained_variance   | -0.0228    |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 6.39       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.00869   |\n",
      "|    value_loss           | 41.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 241         |\n",
      "|    ep_rew_mean          | -71.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 1736        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016956884 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.81       |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00583    |\n",
      "|    value_loss           | 132         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 208        |\n",
      "|    ep_rew_mean          | -76.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 17         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 116        |\n",
      "|    total_timesteps      | 1984       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03474016 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.93      |\n",
      "|    explained_variance   | 0.39       |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 12.1       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    value_loss           | 66         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 186       |\n",
      "|    ep_rew_mean          | -77.5     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 16        |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 131       |\n",
      "|    total_timesteps      | 2232      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0097173 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.82     |\n",
      "|    explained_variance   | 0.224     |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | 82.9      |\n",
      "|    n_updates            | 80        |\n",
      "|    policy_gradient_loss | -0.029    |\n",
      "|    value_loss           | 624       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 169         |\n",
      "|    ep_rew_mean          | -79.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 2480        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012573732 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 490         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00894    |\n",
      "|    value_loss           | 768         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 177         |\n",
      "|    ep_rew_mean          | -81.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 2728        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025984615 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.85       |\n",
      "|    explained_variance   | -1.36       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 658         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.05        |\n",
      "|    value_loss           | 1.49e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 180         |\n",
      "|    ep_rew_mean          | -86.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 2976        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021517742 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 70.5        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00465    |\n",
      "|    value_loss           | 1.31e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 184         |\n",
      "|    ep_rew_mean          | -85.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 195         |\n",
      "|    total_timesteps      | 3224        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021001903 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.98       |\n",
      "|    explained_variance   | -0.448      |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 49.1        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 373         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 184         |\n",
      "|    ep_rew_mean          | -85.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 211         |\n",
      "|    total_timesteps      | 3472        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046425637 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2          |\n",
      "|    explained_variance   | -2.46       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 4.96        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 62.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 206        |\n",
      "|    ep_rew_mean          | -86.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 228        |\n",
      "|    total_timesteps      | 3720       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06298048 |\n",
      "|    clip_fraction        | 0.514      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.06      |\n",
      "|    explained_variance   | -0.552     |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 2.75       |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.00417   |\n",
      "|    value_loss           | 11.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 206         |\n",
      "|    ep_rew_mean          | -86.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 243         |\n",
      "|    total_timesteps      | 3968        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007057696 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.04       |\n",
      "|    explained_variance   | -0.0474     |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 113         |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 1.14e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 206         |\n",
      "|    ep_rew_mean          | -86.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 259         |\n",
      "|    total_timesteps      | 4216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028915009 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.03       |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 3.71        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    value_loss           | 30.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 206         |\n",
      "|    ep_rew_mean          | -86.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 275         |\n",
      "|    total_timesteps      | 4464        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018468097 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.96       |\n",
      "|    explained_variance   | 0.6         |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 3.45        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 67.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 230       |\n",
      "|    ep_rew_mean          | -84.6     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 16        |\n",
      "|    iterations           | 19        |\n",
      "|    time_elapsed         | 291       |\n",
      "|    total_timesteps      | 4712      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0355598 |\n",
      "|    clip_fraction        | 0.3       |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.03     |\n",
      "|    explained_variance   | -0.034    |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | 8.54      |\n",
      "|    n_updates            | 180       |\n",
      "|    policy_gradient_loss | -0.0452   |\n",
      "|    value_loss           | 31.2      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 230         |\n",
      "|    ep_rew_mean          | -84.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 306         |\n",
      "|    total_timesteps      | 4960        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020281615 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.05       |\n",
      "|    explained_variance   | -0.176      |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 48.1        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 394         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 244         |\n",
      "|    ep_rew_mean          | -83.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 5208        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014265655 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.06       |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 21.8        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    value_loss           | 90.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 244        |\n",
      "|    ep_rew_mean          | -85.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 338        |\n",
      "|    total_timesteps      | 5456       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10374028 |\n",
      "|    clip_fraction        | 0.255      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.03      |\n",
      "|    explained_variance   | 0.181      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 78.3       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | 0.0722     |\n",
      "|    value_loss           | 1.33e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 244         |\n",
      "|    ep_rew_mean          | -85.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 354         |\n",
      "|    total_timesteps      | 5704        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012513274 |\n",
      "|    clip_fraction        | 0.0649      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.06       |\n",
      "|    explained_variance   | 0.0822      |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 21.6        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00983    |\n",
      "|    value_loss           | 403         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 244         |\n",
      "|    ep_rew_mean          | -85.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 371         |\n",
      "|    total_timesteps      | 5952        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009337842 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.08       |\n",
      "|    explained_variance   | 0.0132      |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 9.02        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 31          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 244         |\n",
      "|    ep_rew_mean          | -85.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 387         |\n",
      "|    total_timesteps      | 6200        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011134781 |\n",
      "|    clip_fraction        | 0.0194      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.08       |\n",
      "|    explained_variance   | -0.0456     |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 7.37        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00511    |\n",
      "|    value_loss           | 38.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 268         |\n",
      "|    ep_rew_mean          | -86.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 403         |\n",
      "|    total_timesteps      | 6448        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016427573 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.05       |\n",
      "|    explained_variance   | 0.0249      |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 9.36        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    value_loss           | 33.1        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1e6,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best model and get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = './saved_models/PPO/callback0/best_model_PPO/'  # For A2C agent: './saved_models/A2C/callback0/best_model_A2C/'\n",
    "# model = PPO.load(path, env=env_test,device = 'cuda')  # For A2C agent: A2C.load(path, env=env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_statistic(model, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir ./tensorboard_logs_discr/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
